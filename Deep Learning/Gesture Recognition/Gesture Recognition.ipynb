{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# from scipy.misc import imread, imresize\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import imageio\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# tf.random.set_random_seed(30)\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 160\n",
    "img_width = 160\n",
    "channels = 3\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = np.arange(0,30,3)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = (image - np.percentile(image,5)) / (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                   \n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv3D architecture - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model for gesture recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_random_affine():\n",
    "    dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_generator(source_path, folder_list, batch_size):\n",
    "      \n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = [x for x in range(0,nb_frames)] #create a list of image numbers you want to use for a particular video\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "  \n",
    "            batch_data_aug2 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug2 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    " \n",
    "            batch_data_aug3 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug3 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                M2 = get_random_affine()\n",
    "                M3 = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes   \n",
    "                    resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug2[folder,idx] = cv2.warpAffine(resized, M2, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug3[folder,idx] = cv2.warpAffine(resized, M3, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug2[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug3[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug2, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug3, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug2, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug3, axis = 0)\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            \n",
    "            batch_data = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "  \n",
    "            batch_data_aug2 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug2 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "   \n",
    "            batch_data_aug3 = np.zeros((batch_size,nb_frames,nb_rows,nb_cols,nb_channel)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug3 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                M2 = get_random_affine()\n",
    "                M3 = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    resized = cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug2[folder,idx] = cv2.warpAffine(resized, M2, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug3[folder,idx] = cv2.warpAffine(resized, M3, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug2[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug3[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug2, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug3, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug2, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug3, axis = 0) \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_filters = [8,16,32,64]\n",
    "nb_dense = [1000, 500, 5]\n",
    "# input_shape = (30, 120, 120, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 15, 15, 64)     256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              3137000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2505      \n",
      "=================================================================\n",
      "Total params: 3,667,749\n",
      "Trainable params: 3,667,509\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "\n",
    "nb_frames = 30 # number of frames\n",
    "nb_rows = 120 # image width\n",
    "nb_cols = 120 # image height \n",
    "\n",
    "nb_classes = 5\n",
    "nb_channel = 3\n",
    "\n",
    "input_shape=(30,120,120,3)\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(nb_filters[0], kernel_size=(3,3,3), input_shape=(30,120,120,3),\n",
    "                 padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[1], kernel_size=(3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[2], kernel_size=(1,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(nb_filters[3], kernel_size=(1,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(nb_dense[0], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_dense[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(nb_dense[2], activation='softmax'))\n",
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "# train_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/train'\n",
    "# val_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "batch_size = 10\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0329 15:22:30.157113 139998542677824 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, cooldown=1, verbose=1)# write the Reducelronplateau code here\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0329 15:22:36.620363 139998542677824 deprecation.py:323] From <ipython-input-12-6667ca054671>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 10\n",
      "Epoch 1/20\n",
      "65/67 [============================>.] - ETA: 6s - loss: 2.6540 - categorical_accuracy: 0.3335Batch:  67 Index: 10\n",
      "67/67 [==============================] - ETA: 0s - loss: 2.6293 - categorical_accuracy: 0.3341Source path =  Project_data/val ; batch size = 10\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2915_22_30.137713/model-00001-2.62928-0.33409-9.47401-0.23000.h5\n",
      "67/67 [==============================] - 256s 4s/step - loss: 2.6293 - categorical_accuracy: 0.3341 - val_loss: 9.4740 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.6713 - categorical_accuracy: 0.3420\n",
      "Epoch 00002: saving model to model_init_2021-03-2915_22_30.137713/model-00002-1.67134-0.34204-1.21487-0.58000.h5\n",
      "67/67 [==============================] - 67s 1s/step - loss: 1.6713 - categorical_accuracy: 0.3420 - val_loss: 1.2149 - val_categorical_accuracy: 0.5800 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4938 - categorical_accuracy: 0.3657\n",
      "Epoch 00003: saving model to model_init_2021-03-2915_22_30.137713/model-00003-1.49376-0.36567-1.50779-0.34250.h5\n",
      "67/67 [==============================] - 66s 981ms/step - loss: 1.4938 - categorical_accuracy: 0.3657 - val_loss: 1.5078 - val_categorical_accuracy: 0.3425 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4035 - categorical_accuracy: 0.3955\n",
      "Epoch 00004: saving model to model_init_2021-03-2915_22_30.137713/model-00004-1.40351-0.39552-1.13348-0.50000.h5\n",
      "67/67 [==============================] - 65s 973ms/step - loss: 1.4035 - categorical_accuracy: 0.3955 - val_loss: 1.1335 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4044 - categorical_accuracy: 0.4080\n",
      "Epoch 00005: saving model to model_init_2021-03-2915_22_30.137713/model-00005-1.40435-0.40796-1.34120-0.43250.h5\n",
      "67/67 [==============================] - 66s 990ms/step - loss: 1.4044 - categorical_accuracy: 0.4080 - val_loss: 1.3412 - val_categorical_accuracy: 0.4325 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.2937 - categorical_accuracy: 0.4776\n",
      "Epoch 00006: saving model to model_init_2021-03-2915_22_30.137713/model-00006-1.29369-0.47761-1.15542-0.47750.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "67/67 [==============================] - 64s 950ms/step - loss: 1.2937 - categorical_accuracy: 0.4776 - val_loss: 1.1554 - val_categorical_accuracy: 0.4775 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0809 - categorical_accuracy: 0.5547\n",
      "Epoch 00007: saving model to model_init_2021-03-2915_22_30.137713/model-00007-1.08092-0.55473-1.14964-0.56000.h5\n",
      "67/67 [==============================] - 66s 990ms/step - loss: 1.0809 - categorical_accuracy: 0.5547 - val_loss: 1.1496 - val_categorical_accuracy: 0.5600 - lr: 5.0000e-04\n",
      "Epoch 8/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.1379 - categorical_accuracy: 0.5510\n",
      "Epoch 00008: saving model to model_init_2021-03-2915_22_30.137713/model-00008-1.13789-0.55100-1.18160-0.53000.h5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "67/67 [==============================] - 66s 980ms/step - loss: 1.1379 - categorical_accuracy: 0.5510 - val_loss: 1.1816 - val_categorical_accuracy: 0.5300 - lr: 5.0000e-04\n",
      "Epoch 9/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0117 - categorical_accuracy: 0.5983\n",
      "Epoch 00009: saving model to model_init_2021-03-2915_22_30.137713/model-00009-1.01172-0.59826-1.05728-0.56000.h5\n",
      "67/67 [==============================] - 64s 957ms/step - loss: 1.0117 - categorical_accuracy: 0.5983 - val_loss: 1.0573 - val_categorical_accuracy: 0.5600 - lr: 2.5000e-04\n",
      "Epoch 10/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9189 - categorical_accuracy: 0.6455\n",
      "Epoch 00010: saving model to model_init_2021-03-2915_22_30.137713/model-00010-0.91889-0.64552-1.01841-0.61500.h5\n",
      "67/67 [==============================] - 67s 995ms/step - loss: 0.9189 - categorical_accuracy: 0.6455 - val_loss: 1.0184 - val_categorical_accuracy: 0.6150 - lr: 2.5000e-04\n",
      "Epoch 11/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.9276 - categorical_accuracy: 0.6393\n",
      "Epoch 00011: saving model to model_init_2021-03-2915_22_30.137713/model-00011-0.92761-0.63930-0.82445-0.66500.h5\n",
      "67/67 [==============================] - 65s 964ms/step - loss: 0.9276 - categorical_accuracy: 0.6393 - val_loss: 0.8245 - val_categorical_accuracy: 0.6650 - lr: 2.5000e-04\n",
      "Epoch 12/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.8593 - categorical_accuracy: 0.6692\n",
      "Epoch 00012: saving model to model_init_2021-03-2915_22_30.137713/model-00012-0.85929-0.66915-0.74496-0.71500.h5\n",
      "67/67 [==============================] - 65s 975ms/step - loss: 0.8593 - categorical_accuracy: 0.6692 - val_loss: 0.7450 - val_categorical_accuracy: 0.7150 - lr: 2.5000e-04\n",
      "Epoch 13/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7960 - categorical_accuracy: 0.6729\n",
      "Epoch 00013: saving model to model_init_2021-03-2915_22_30.137713/model-00013-0.79596-0.67289-0.85207-0.68750.h5\n",
      "67/67 [==============================] - 64s 961ms/step - loss: 0.7960 - categorical_accuracy: 0.6729 - val_loss: 0.8521 - val_categorical_accuracy: 0.6875 - lr: 2.5000e-04\n",
      "Epoch 14/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.8821 - categorical_accuracy: 0.6604\n",
      "Epoch 00014: saving model to model_init_2021-03-2915_22_30.137713/model-00014-0.88214-0.66045-0.69430-0.80500.h5\n",
      "67/67 [==============================] - 64s 956ms/step - loss: 0.8821 - categorical_accuracy: 0.6604 - val_loss: 0.6943 - val_categorical_accuracy: 0.8050 - lr: 2.5000e-04\n",
      "Epoch 15/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7800 - categorical_accuracy: 0.7239\n",
      "Epoch 00015: saving model to model_init_2021-03-2915_22_30.137713/model-00015-0.78000-0.72388-0.67242-0.78750.h5\n",
      "67/67 [==============================] - 64s 962ms/step - loss: 0.7800 - categorical_accuracy: 0.7239 - val_loss: 0.6724 - val_categorical_accuracy: 0.7875 - lr: 2.5000e-04\n",
      "Epoch 16/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.6574 - categorical_accuracy: 0.7438\n",
      "Epoch 00016: saving model to model_init_2021-03-2915_22_30.137713/model-00016-0.65737-0.74378-0.83526-0.69000.h5\n",
      "67/67 [==============================] - 64s 949ms/step - loss: 0.6574 - categorical_accuracy: 0.7438 - val_loss: 0.8353 - val_categorical_accuracy: 0.6900 - lr: 2.5000e-04\n",
      "Epoch 17/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7172 - categorical_accuracy: 0.7450\n",
      "Epoch 00017: saving model to model_init_2021-03-2915_22_30.137713/model-00017-0.71724-0.74502-0.81301-0.70500.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "67/67 [==============================] - 65s 968ms/step - loss: 0.7172 - categorical_accuracy: 0.7450 - val_loss: 0.8130 - val_categorical_accuracy: 0.7050 - lr: 2.5000e-04\n",
      "Epoch 18/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.7291 - categorical_accuracy: 0.7214\n",
      "Epoch 00018: saving model to model_init_2021-03-2915_22_30.137713/model-00018-0.72912-0.72139-0.71269-0.68000.h5\n",
      "67/67 [==============================] - 64s 952ms/step - loss: 0.7291 - categorical_accuracy: 0.7214 - val_loss: 0.7127 - val_categorical_accuracy: 0.6800 - lr: 1.2500e-04\n",
      "Epoch 19/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.5659 - categorical_accuracy: 0.8010\n",
      "Epoch 00019: saving model to model_init_2021-03-2915_22_30.137713/model-00019-0.56588-0.80100-0.67532-0.77250.h5\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "67/67 [==============================] - 64s 957ms/step - loss: 0.5659 - categorical_accuracy: 0.8010 - val_loss: 0.6753 - val_categorical_accuracy: 0.7725 - lr: 1.2500e-04\n",
      "Epoch 20/20\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.6731 - categorical_accuracy: 0.7475\n",
      "Epoch 00020: saving model to model_init_2021-03-2915_22_30.137713/model-00020-0.67308-0.74751-0.67062-0.72000.h5\n",
      "67/67 [==============================] - 64s 960ms/step - loss: 0.6731 - categorical_accuracy: 0.7475 - val_loss: 0.6706 - val_categorical_accuracy: 0.7200 - lr: 6.2500e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f53510c8d30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "train_generator = aug_generator(train_path, train_doc, batch_size)\n",
    "val_generator = aug_generator(val_path, val_doc, batch_size)\n",
    "num_epochs = 20\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 -\n",
    "Optimiser - Adam <br>\n",
    "Grayscale image (ie only one channel is used) is used <br>\n",
    "Image is resized to the size 160/160\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 (that too the frame from 0 to 31) <br>\n",
    "Architecture - Architecture 1\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 160\n",
    "img_width = 160\n",
    "channels = 1\n",
    "img_idx = np.arange(0,30,3)\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_height = 160\n",
    "    img_width = 160\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        # num_batches = # calculate the number of batches\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = image.convert('L')\n",
    "                    image = (image - np.percentile(image,5)) / (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                   \n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = image.convert('L')\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "          input_shape=(len(img_idx),img_height,img_width,channels)))\n",
    "\n",
    "\n",
    "model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_10 (Conv3D)           (None, 10, 160, 160, 16)  448       \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 10, 160, 160, 64)  8256      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10, 160, 160, 64)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 10, 160, 160, 64)  256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 5, 80, 80, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 5, 80, 80, 128)    65664     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 5, 80, 80, 128)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 5, 80, 80, 128)    512       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 2, 40, 40, 128)    0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 409600)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               52428928  \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 52,513,413\n",
      "Trainable params: 52,512,645\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'Adam'\n",
    "\n",
    "# compile it\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['categorical_accuracy'])\n",
    "\n",
    "# summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0329 12:29:43.155325 139856429156160 deprecation.py:323] From <ipython-input-37-bd77c9c60c14>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 16\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.1575 - categorical_accuracy: 0.2564Source path =  Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_2021-03-2912_29_39.544224/model-00001-2.15752-0.25641-13.80201-0.30000.h5\n",
      "42/42 [==============================] - 57s 1s/step - loss: 2.1575 - categorical_accuracy: 0.2564 - val_loss: 13.8020 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.7219 - categorical_accuracy: 0.3650\n",
      "Epoch 00002: saving model to model_2021-03-2912_29_39.544224/model-00002-1.72195-0.36501-28.94143-0.23000.h5\n",
      "42/42 [==============================] - 55s 1s/step - loss: 1.7219 - categorical_accuracy: 0.3650 - val_loss: 28.9414 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.3528 - categorical_accuracy: 0.4751\n",
      "Epoch 00003: saving model to model_2021-03-2912_29_39.544224/model-00003-1.35284-0.47511-62.38010-0.20000.h5\n",
      "42/42 [==============================] - 54s 1s/step - loss: 1.3528 - categorical_accuracy: 0.4751 - val_loss: 62.3801 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2362 - categorical_accuracy: 0.5204\n",
      "Epoch 00004: saving model to model_2021-03-2912_29_39.544224/model-00004-1.23616-0.52036-65.45608-0.20000.h5\n",
      "42/42 [==============================] - 54s 1s/step - loss: 1.2362 - categorical_accuracy: 0.5204 - val_loss: 65.4561 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2329 - categorical_accuracy: 0.5098\n",
      "Epoch 00005: saving model to model_2021-03-2912_29_39.544224/model-00005-1.23290-0.50980-12.73339-0.27000.h5\n",
      "42/42 [==============================] - 55s 1s/step - loss: 1.2329 - categorical_accuracy: 0.5098 - val_loss: 12.7334 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1226 - categorical_accuracy: 0.5566\n",
      "Epoch 00006: saving model to model_2021-03-2912_29_39.544224/model-00006-1.12259-0.55656-11.80759-0.24000.h5\n",
      "42/42 [==============================] - 55s 1s/step - loss: 1.1226 - categorical_accuracy: 0.5566 - val_loss: 11.8076 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9621 - categorical_accuracy: 0.6440\n",
      "Epoch 00007: saving model to model_2021-03-2912_29_39.544224/model-00007-0.96210-0.64404-11.15958-0.23000.h5\n",
      "42/42 [==============================] - 54s 1s/step - loss: 0.9621 - categorical_accuracy: 0.6440 - val_loss: 11.1596 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8266 - categorical_accuracy: 0.6802\n",
      "Epoch 00008: saving model to model_2021-03-2912_29_39.544224/model-00008-0.82656-0.68024-1.53739-0.42000.h5\n",
      "42/42 [==============================] - 54s 1s/step - loss: 0.8266 - categorical_accuracy: 0.6802 - val_loss: 1.5374 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6991 - categorical_accuracy: 0.7421\n",
      "Epoch 00009: saving model to model_2021-03-2912_29_39.544224/model-00009-0.69909-0.74208-1.78862-0.21000.h5\n",
      "42/42 [==============================] - 55s 1s/step - loss: 0.6991 - categorical_accuracy: 0.7421 - val_loss: 1.7886 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6205 - categorical_accuracy: 0.7617\n",
      "Epoch 00010: saving model to model_2021-03-2912_29_39.544224/model-00010-0.62048-0.76169-1.88751-0.17000.h5\n",
      "42/42 [==============================] - 55s 1s/step - loss: 0.6205 - categorical_accuracy: 0.7617 - val_loss: 1.8875 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f31c02782e8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel is used) is used <br>\n",
    "Image is cropped to the size 160/160 (for the image of 120x160 black padding will be provided)\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 ( the frame from 0 to 30) <br>\n",
    "Architecture - Architecture 1\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 160\n",
    "img_width = 160\n",
    "channels = 3\n",
    "# img_idx = np.arange(10,21,2)\n",
    "img_idx = np.arange(0,30,3)\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-80,mid_height-80,mid_width+80,mid_height+80))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                   \n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-80,mid_height-80,mid_width+80,mid_height+80))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same',\n",
    "          input_shape=(len(img_idx),img_height,img_width,channels)))\n",
    "\n",
    "\n",
    "model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_8 (Conv3D)            (None, 6, 160, 160, 32)   2624      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 6, 160, 160, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 6, 160, 160, 32)   27680     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 6, 160, 160, 32)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 2, 54, 54, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 54, 54, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 2, 54, 54, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 2, 54, 54, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 2, 54, 54, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 2, 54, 54, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2, 54, 54, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 1, 18, 18, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               10617344  \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 10,818,533\n",
      "Trainable params: 10,817,381\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'Adam'\n",
    "\n",
    "# compile it\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['categorical_accuracy'])\n",
    "\n",
    "# summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 64\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.4385 - categorical_accuracy: 0.3002Source path =  Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: saving model to model_2021-03-2719_43_37.784431/model-00001-2.43851-0.30015-1.66938-0.31000.h5\n",
      "11/11 [==============================] - 37s 3s/step - loss: 2.4385 - categorical_accuracy: 0.3002 - val_loss: 1.6694 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.5930 - categorical_accuracy: 0.3952\n",
      "Epoch 00002: saving model to model_2021-03-2719_43_37.784431/model-00002-1.59296-0.39517-1.78173-0.23000.h5\n",
      "11/11 [==============================] - 32s 3s/step - loss: 1.5930 - categorical_accuracy: 0.3952 - val_loss: 1.7817 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3651 - categorical_accuracy: 0.4691\n",
      "Epoch 00003: saving model to model_2021-03-2719_43_37.784431/model-00003-1.36509-0.46908-1.73309-0.29000.h5\n",
      "11/11 [==============================] - 33s 3s/step - loss: 1.3651 - categorical_accuracy: 0.4691 - val_loss: 1.7331 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1453 - categorical_accuracy: 0.5762\n",
      "Epoch 00004: saving model to model_2021-03-2719_43_37.784431/model-00004-1.14526-0.57617-1.80857-0.30000.h5\n",
      "11/11 [==============================] - 32s 3s/step - loss: 1.1453 - categorical_accuracy: 0.5762 - val_loss: 1.8086 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0044 - categorical_accuracy: 0.6320\n",
      "Epoch 00005: saving model to model_2021-03-2719_43_37.784431/model-00005-1.00442-0.63198-2.52000-0.21000.h5\n",
      "11/11 [==============================] - 33s 3s/step - loss: 1.0044 - categorical_accuracy: 0.6320 - val_loss: 2.5200 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8413 - categorical_accuracy: 0.6938\n",
      "Epoch 00006: saving model to model_2021-03-2719_43_37.784431/model-00006-0.84128-0.69382-2.71930-0.18000.h5\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.8413 - categorical_accuracy: 0.6938 - val_loss: 2.7193 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7083 - categorical_accuracy: 0.7481\n",
      "Epoch 00007: saving model to model_2021-03-2719_43_37.784431/model-00007-0.70830-0.74811-3.24413-0.26000.h5\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.7083 - categorical_accuracy: 0.7481 - val_loss: 3.2441 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5345 - categorical_accuracy: 0.8145\n",
      "Epoch 00008: saving model to model_2021-03-2719_43_37.784431/model-00008-0.53451-0.81448-3.96322-0.18000.h5\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.5345 - categorical_accuracy: 0.8145 - val_loss: 3.9632 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4802 - categorical_accuracy: 0.8311\n",
      "Epoch 00009: saving model to model_2021-03-2719_43_37.784431/model-00009-0.48017-0.83107-4.31341-0.22000.h5\n",
      "11/11 [==============================] - 33s 3s/step - loss: 0.4802 - categorical_accuracy: 0.8311 - val_loss: 4.3134 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4465 - categorical_accuracy: 0.8371\n",
      "Epoch 00010: saving model to model_2021-03-2719_43_37.784431/model-00010-0.44649-0.83710-4.94111-0.19000.h5\n",
      "11/11 [==============================] - 32s 3s/step - loss: 0.4465 - categorical_accuracy: 0.8371 - val_loss: 4.9411 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3f0418f8d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 4 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel is used) is used <br>\n",
    "Image is first resized to the standard size of 120x160 and then cropped to a size of 80x120\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 ( the frame from 0 to 30) <br>\n",
    "Architecture - Architecture 1\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 80\n",
    "img_width = 120\n",
    "channels = 3\n",
    "# img_idx = np.arange(10,21,2)\n",
    "img_idx = np.arange(0,30,3)\n",
    "\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    if image.height==360 and image.width==360:\n",
    "                        image = image.resize((160,120))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-60,mid_height-40,mid_width+60,mid_height+40))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    if image.height==360 and image.width==360:\n",
    "                        image = image.resize((160,120))\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-60,mid_height-40,mid_width+60,mid_height+40))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(16, (2, 2, 2), padding='same',\n",
    "          input_shape=(len(img_idx),img_height,img_width,channels)))\n",
    "\n",
    "\n",
    "model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# model.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_4 (Conv3D)            (None, 6, 80, 120, 32)    2624      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6, 80, 120, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 6, 80, 120, 32)    27680     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6, 80, 120, 32)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 27, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2, 27, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 2, 27, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2, 27, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 2, 27, 40, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2, 27, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2, 27, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 9, 14, 64)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8064)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               4129280   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 4,330,469\n",
      "Trainable params: 4,329,317\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'Adam'\n",
    "\n",
    "# compile it\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['categorical_accuracy'])\n",
    "\n",
    "# summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0327 19:38:33.229362 139911367006016 deprecation.py:323] From <ipython-input-12-bd77c9c60c14>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 64\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.4001 - categorical_accuracy: 0.3183Source path =  Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: saving model to model_2021-03-2719_33_57.452747/model-00001-2.40011-0.31825-2.28037-0.24000.h5\n",
      "11/11 [==============================] - 52s 5s/step - loss: 2.4001 - categorical_accuracy: 0.3183 - val_loss: 2.2804 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.6195 - categorical_accuracy: 0.3861\n",
      "Epoch 00002: saving model to model_2021-03-2719_33_57.452747/model-00002-1.61950-0.38612-1.65325-0.25000.h5\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.6195 - categorical_accuracy: 0.3861 - val_loss: 1.6532 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3221 - categorical_accuracy: 0.4857\n",
      "Epoch 00003: saving model to model_2021-03-2719_33_57.452747/model-00003-1.32212-0.48567-1.96197-0.21000.h5\n",
      "11/11 [==============================] - 17s 2s/step - loss: 1.3221 - categorical_accuracy: 0.4857 - val_loss: 1.9620 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1754 - categorical_accuracy: 0.5400\n",
      "Epoch 00004: saving model to model_2021-03-2719_33_57.452747/model-00004-1.17540-0.53997-1.88818-0.40000.h5\n",
      "11/11 [==============================] - 16s 1s/step - loss: 1.1754 - categorical_accuracy: 0.5400 - val_loss: 1.8882 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0863 - categorical_accuracy: 0.6094\n",
      "Epoch 00005: saving model to model_2021-03-2719_33_57.452747/model-00005-1.08628-0.60935-1.87936-0.38000.h5\n",
      "11/11 [==============================] - 17s 2s/step - loss: 1.0863 - categorical_accuracy: 0.6094 - val_loss: 1.8794 - val_categorical_accuracy: 0.3800 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8861 - categorical_accuracy: 0.6501\n",
      "Epoch 00006: saving model to model_2021-03-2719_33_57.452747/model-00006-0.88605-0.65008-2.26286-0.26000.h5\n",
      "11/11 [==============================] - 16s 1s/step - loss: 0.8861 - categorical_accuracy: 0.6501 - val_loss: 2.2629 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8091 - categorical_accuracy: 0.6802\n",
      "Epoch 00007: saving model to model_2021-03-2719_33_57.452747/model-00007-0.80909-0.68024-2.04607-0.31000.h5\n",
      "11/11 [==============================] - 17s 2s/step - loss: 0.8091 - categorical_accuracy: 0.6802 - val_loss: 2.0461 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6560 - categorical_accuracy: 0.7557\n",
      "Epoch 00008: saving model to model_2021-03-2719_33_57.452747/model-00008-0.65599-0.75566-2.52142-0.30000.h5\n",
      "11/11 [==============================] - 16s 1s/step - loss: 0.6560 - categorical_accuracy: 0.7557 - val_loss: 2.5214 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5410 - categorical_accuracy: 0.7888\n",
      "Epoch 00009: saving model to model_2021-03-2719_33_57.452747/model-00009-0.54098-0.78884-2.58591-0.34000.h5\n",
      "11/11 [==============================] - 17s 2s/step - loss: 0.5410 - categorical_accuracy: 0.7888 - val_loss: 2.5859 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4798 - categorical_accuracy: 0.8145\n",
      "Epoch 00010: saving model to model_2021-03-2719_33_57.452747/model-00010-0.47978-0.81448-3.11786-0.28000.h5\n",
      "11/11 [==============================] - 16s 1s/step - loss: 0.4798 - categorical_accuracy: 0.8145 - val_loss: 3.1179 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3f0c042780>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 5 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel is used) is used <br>\n",
    "Image is resized and then cropped to the standard size of 80x120<br>\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 6 ( the frame from 10 to 20) <br>\n",
    "Architecture - Architecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 160\n",
    "img_width = 160\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = np.arange(10,21,2)\n",
    "\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    if image.height==360 and image.width==360:\n",
    "                        image = image.resize((160,160))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-80,mid_height-80,mid_width+80,mid_height+80))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                   \n",
    "                    if image.height==360 and image.width==360:\n",
    "                        image = image.resize((160,160))\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-80,mid_height-80,mid_width+80,mid_height+80))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "\n",
    "img_height = 160\n",
    "img_width = 160\n",
    "channels = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same', input_shape=(6,img_height,img_width,channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 6, 160, 160, 32)   2624      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 6, 160, 160, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 6, 160, 160, 32)   27680     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6, 160, 160, 32)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6, 160, 160, 32)   128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 2, 54, 54, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 54, 54, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 2, 54, 54, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 54, 54, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 54, 54, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 54, 54, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 54, 54, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 1, 18, 18, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               10617344  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 10,818,661\n",
      "Trainable params: 10,817,445\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'Adam'\n",
    "\n",
    "# compile it\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['categorical_accuracy'])\n",
    "\n",
    "# summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0329 13:06:19.899816 139947362854720 deprecation.py:323] From <ipython-input-13-bd77c9c60c14>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 32\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.0367 - categorical_accuracy: 0.3786Source path =  Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_2021-03-2913_05_37.548373/model-00001-2.03669-0.37858-2.10024-0.21000.h5\n",
      "21/21 [==============================] - 69s 3s/step - loss: 2.0367 - categorical_accuracy: 0.3786 - val_loss: 2.1002 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1827 - categorical_accuracy: 0.5505\n",
      "Epoch 00002: saving model to model_2021-03-2913_05_37.548373/model-00002-1.18269-0.55053-2.78735-0.32000.h5\n",
      "21/21 [==============================] - 44s 2s/step - loss: 1.1827 - categorical_accuracy: 0.5505 - val_loss: 2.7874 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8140 - categorical_accuracy: 0.6878\n",
      "Epoch 00003: saving model to model_2021-03-2913_05_37.548373/model-00003-0.81399-0.68778-8.17916-0.28000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.8140 - categorical_accuracy: 0.6878 - val_loss: 8.1792 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6090 - categorical_accuracy: 0.7587\n",
      "Epoch 00004: saving model to model_2021-03-2913_05_37.548373/model-00004-0.60898-0.75867-13.53042-0.24000.h5\n",
      "21/21 [==============================] - 42s 2s/step - loss: 0.6090 - categorical_accuracy: 0.7587 - val_loss: 13.5304 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4779 - categorical_accuracy: 0.8130\n",
      "Epoch 00005: saving model to model_2021-03-2913_05_37.548373/model-00005-0.47789-0.81297-16.15639-0.23000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.4779 - categorical_accuracy: 0.8130 - val_loss: 16.1564 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3645 - categorical_accuracy: 0.8612\n",
      "Epoch 00006: saving model to model_2021-03-2913_05_37.548373/model-00006-0.36451-0.86124-20.69443-0.21000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.3645 - categorical_accuracy: 0.8612 - val_loss: 20.6944 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2393 - categorical_accuracy: 0.9186\n",
      "Epoch 00007: saving model to model_2021-03-2913_05_37.548373/model-00007-0.23929-0.91855-26.54247-0.22000.h5\n",
      "21/21 [==============================] - 44s 2s/step - loss: 0.2393 - categorical_accuracy: 0.9186 - val_loss: 26.5425 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1537 - categorical_accuracy: 0.9548\n",
      "Epoch 00008: saving model to model_2021-03-2913_05_37.548373/model-00008-0.15373-0.95475-27.51917-0.23000.h5\n",
      "21/21 [==============================] - 42s 2s/step - loss: 0.1537 - categorical_accuracy: 0.9548 - val_loss: 27.5192 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1329 - categorical_accuracy: 0.9532\n",
      "Epoch 00009: saving model to model_2021-03-2913_05_37.548373/model-00009-0.13285-0.95324-16.17052-0.22000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.1329 - categorical_accuracy: 0.9532 - val_loss: 16.1705 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0965 - categorical_accuracy: 0.9729\n",
      "Epoch 00010: saving model to model_2021-03-2913_05_37.548373/model-00010-0.09645-0.97285-17.09634-0.21000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.0965 - categorical_accuracy: 0.9729 - val_loss: 17.0963 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f474c4dbac8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 6 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel) are used <br>\n",
    "Image is resized to the size 160/160\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 (that too the frame from 0 to 31) <br>\n",
    "batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 160\n",
    "img_width = 160\n",
    "channels = 3\n",
    "img_idx = np.arange(0,30,1)\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = (image - np.percentile(image,5)) / (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                   \n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=(len(img_idx),img_height,img_width,channels), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_12 (Conv3D)           (None, 30, 160, 160, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 30, 160, 160, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 30, 160, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 30, 160, 160, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 15, 80, 80, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 15, 80, 80, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 15, 80, 80, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 15, 80, 80, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 7, 40, 40, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 7, 40, 40, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 7, 40, 40, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_12 (MaxPooling (None, 3, 20, 20, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 3, 20, 20, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 3, 20, 20, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_13 (MaxPooling (None, 1, 10, 10, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               1638656   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,699,685\n",
      "Trainable params: 1,699,573\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"Adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0329 13:18:14.023926 139947362854720 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 16\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.5184 - categorical_accuracy: 0.2172Source path =  Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2913_18_11.848691/model-00001-2.51845-0.21719-1.60724-0.21000.h5\n",
      "42/42 [==============================] - 181s 4s/step - loss: 2.5184 - categorical_accuracy: 0.2172 - val_loss: 1.6072 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.4991 - categorical_accuracy: 0.3363\n",
      "Epoch 00002: saving model to model_init_2021-03-2913_18_11.848691/model-00002-1.49911-0.33635-1.58685-0.32000.h5\n",
      "42/42 [==============================] - 113s 3s/step - loss: 1.4991 - categorical_accuracy: 0.3363 - val_loss: 1.5868 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.3343 - categorical_accuracy: 0.4314\n",
      "Epoch 00003: saving model to model_init_2021-03-2913_18_11.848691/model-00003-1.33431-0.43137-1.62208-0.22000.h5\n",
      "42/42 [==============================] - 113s 3s/step - loss: 1.3343 - categorical_accuracy: 0.4314 - val_loss: 1.6221 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1540 - categorical_accuracy: 0.5581\n",
      "Epoch 00004: saving model to model_init_2021-03-2913_18_11.848691/model-00004-1.15405-0.55807-1.63866-0.23000.h5\n",
      "42/42 [==============================] - 113s 3s/step - loss: 1.1540 - categorical_accuracy: 0.5581 - val_loss: 1.6387 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9820 - categorical_accuracy: 0.5837\n",
      "Epoch 00005: saving model to model_init_2021-03-2913_18_11.848691/model-00005-0.98203-0.58371-1.73103-0.22000.h5\n",
      "42/42 [==============================] - 114s 3s/step - loss: 0.9820 - categorical_accuracy: 0.5837 - val_loss: 1.7310 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8929 - categorical_accuracy: 0.6486\n",
      "Epoch 00006: saving model to model_init_2021-03-2913_18_11.848691/model-00006-0.89290-0.64857-1.82701-0.28000.h5\n",
      "42/42 [==============================] - 113s 3s/step - loss: 0.8929 - categorical_accuracy: 0.6486 - val_loss: 1.8270 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8573 - categorical_accuracy: 0.6742\n",
      "Epoch 00007: saving model to model_init_2021-03-2913_18_11.848691/model-00007-0.85732-0.67421-1.84246-0.27000.h5\n",
      "42/42 [==============================] - 110s 3s/step - loss: 0.8573 - categorical_accuracy: 0.6742 - val_loss: 1.8425 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7132 - categorical_accuracy: 0.7210\n",
      "Epoch 00008: saving model to model_init_2021-03-2913_18_11.848691/model-00008-0.71323-0.72097-1.63604-0.31000.h5\n",
      "42/42 [==============================] - 113s 3s/step - loss: 0.7132 - categorical_accuracy: 0.7210 - val_loss: 1.6360 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.5785 - categorical_accuracy: 0.7964\n",
      "Epoch 00009: saving model to model_init_2021-03-2913_18_11.848691/model-00009-0.57848-0.79638-1.35303-0.44000.h5\n",
      "42/42 [==============================] - 113s 3s/step - loss: 0.5785 - categorical_accuracy: 0.7964 - val_loss: 1.3530 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.5165 - categorical_accuracy: 0.8069\n",
      "Epoch 00010: saving model to model_init_2021-03-2913_18_11.848691/model-00010-0.51655-0.80694-1.46606-0.32000.h5\n",
      "42/42 [==============================] - 114s 3s/step - loss: 0.5165 - categorical_accuracy: 0.8069 - val_loss: 1.4661 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f47080ae518>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 7 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel) are used <br>\n",
    "Image is resized to the size 120/120\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 (that too the frame from 0 to 31) <br>\n",
    "batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 120\n",
    "img_width = 120\n",
    "channels = 3\n",
    "img_idx = np.arange(0,30,1)\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = (image - np.percentile(image,5)) / (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                   \n",
    "                    image = image.resize((img_height,img_width))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=(len(img_idx),img_height,img_width,channels), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_16 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_14 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_17 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_15 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_18 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_16 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_19 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_17 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"Adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0329 13:39:40.976706 139947362854720 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 16\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.2737 - categorical_accuracy: 0.2202Source path =  Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2913_39_38.561164/model-00001-2.27375-0.22021-1.60772-0.21000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 2.2737 - categorical_accuracy: 0.2202 - val_loss: 1.6077 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5830 - categorical_accuracy: 0.2700\n",
      "Epoch 00002: saving model to model_init_2021-03-2913_39_38.561164/model-00002-1.58299-0.26998-1.60207-0.20000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 1.5830 - categorical_accuracy: 0.2700 - val_loss: 1.6021 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.4459 - categorical_accuracy: 0.3635\n",
      "Epoch 00003: saving model to model_init_2021-03-2913_39_38.561164/model-00003-1.44586-0.36350-1.59683-0.23000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 1.4459 - categorical_accuracy: 0.3635 - val_loss: 1.5968 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.3473 - categorical_accuracy: 0.4404\n",
      "Epoch 00004: saving model to model_init_2021-03-2913_39_38.561164/model-00004-1.34734-0.44042-1.59678-0.32000.h5\n",
      "42/42 [==============================] - 93s 2s/step - loss: 1.3473 - categorical_accuracy: 0.4404 - val_loss: 1.5968 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2005 - categorical_accuracy: 0.5370\n",
      "Epoch 00005: saving model to model_init_2021-03-2913_39_38.561164/model-00005-1.20051-0.53695-1.56952-0.29000.h5\n",
      "42/42 [==============================] - 91s 2s/step - loss: 1.2005 - categorical_accuracy: 0.5370 - val_loss: 1.5695 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0971 - categorical_accuracy: 0.5490\n",
      "Epoch 00006: saving model to model_init_2021-03-2913_39_38.561164/model-00006-1.09715-0.54902-1.54342-0.31000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 1.0971 - categorical_accuracy: 0.5490 - val_loss: 1.5434 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0128 - categorical_accuracy: 0.6109\n",
      "Epoch 00007: saving model to model_init_2021-03-2913_39_38.561164/model-00007-1.01279-0.61086-1.48458-0.43000.h5\n",
      "42/42 [==============================] - 90s 2s/step - loss: 1.0128 - categorical_accuracy: 0.6109 - val_loss: 1.4846 - val_categorical_accuracy: 0.4300 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9288 - categorical_accuracy: 0.6516\n",
      "Epoch 00008: saving model to model_init_2021-03-2913_39_38.561164/model-00008-0.92885-0.65158-1.40401-0.47000.h5\n",
      "42/42 [==============================] - 91s 2s/step - loss: 0.9288 - categorical_accuracy: 0.6516 - val_loss: 1.4040 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8235 - categorical_accuracy: 0.6908\n",
      "Epoch 00009: saving model to model_init_2021-03-2913_39_38.561164/model-00009-0.82352-0.69080-1.33626-0.44000.h5\n",
      "42/42 [==============================] - 90s 2s/step - loss: 0.8235 - categorical_accuracy: 0.6908 - val_loss: 1.3363 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7151 - categorical_accuracy: 0.7210\n",
      "Epoch 00010: saving model to model_init_2021-03-2913_39_38.561164/model-00010-0.71507-0.72097-1.32340-0.47000.h5\n",
      "42/42 [==============================] - 92s 2s/step - loss: 0.7151 - categorical_accuracy: 0.7210 - val_loss: 1.3234 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46ec419a20>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 8 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel) are used <br>\n",
    "Image is cropped to the size 120/120\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 (that too the frame from 0 to 31) <br>\n",
    "batch size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 120\n",
    "img_width = 120\n",
    "channels = 3\n",
    "img_idx = np.arange(0,30,1)\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    image = (image - np.percentile(image,5)) / (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=(len(img_idx),img_height,img_width,channels), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_20 (Conv3D)           (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_20 (MaxPooling (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_23 (Conv3D)           (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_21 (MaxPooling (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"Adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0329 14:01:18.717895 139947362854720 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 16\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.2759 - categorical_accuracy: 0.2308Source path =  Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2914_01_17.209734/model-00001-2.27595-0.23077-1.60952-0.18000.h5\n",
      "42/42 [==============================] - 95s 2s/step - loss: 2.2759 - categorical_accuracy: 0.2308 - val_loss: 1.6095 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5803 - categorical_accuracy: 0.2700\n",
      "Epoch 00002: saving model to model_init_2021-03-2914_01_17.209734/model-00002-1.58028-0.26998-1.61324-0.19000.h5\n",
      "42/42 [==============================] - 103s 2s/step - loss: 1.5803 - categorical_accuracy: 0.2700 - val_loss: 1.6132 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.4861 - categorical_accuracy: 0.3560\n",
      "Epoch 00003: saving model to model_init_2021-03-2914_01_17.209734/model-00003-1.48608-0.35596-1.61845-0.15000.h5\n",
      "42/42 [==============================] - 122s 3s/step - loss: 1.4861 - categorical_accuracy: 0.3560 - val_loss: 1.6185 - val_categorical_accuracy: 0.1500 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.3497 - categorical_accuracy: 0.4465\n",
      "Epoch 00004: saving model to model_init_2021-03-2914_01_17.209734/model-00004-1.34968-0.44646-1.65570-0.17000.h5\n",
      "42/42 [==============================] - 95s 2s/step - loss: 1.3497 - categorical_accuracy: 0.4465 - val_loss: 1.6557 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2997 - categorical_accuracy: 0.4872\n",
      "Epoch 00005: saving model to model_init_2021-03-2914_01_17.209734/model-00005-1.29971-0.48718-1.59454-0.27000.h5\n",
      "42/42 [==============================] - 104s 2s/step - loss: 1.2997 - categorical_accuracy: 0.4872 - val_loss: 1.5945 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2544 - categorical_accuracy: 0.4977\n",
      "Epoch 00006: saving model to model_init_2021-03-2914_01_17.209734/model-00006-1.25443-0.49774-1.59705-0.24000.h5\n",
      "42/42 [==============================] - 121s 3s/step - loss: 1.2544 - categorical_accuracy: 0.4977 - val_loss: 1.5970 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1653 - categorical_accuracy: 0.5430\n",
      "Epoch 00007: saving model to model_init_2021-03-2914_01_17.209734/model-00007-1.16534-0.54299-1.56821-0.24000.h5\n",
      "42/42 [==============================] - 137s 3s/step - loss: 1.1653 - categorical_accuracy: 0.5430 - val_loss: 1.5682 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9982 - categorical_accuracy: 0.6139\n",
      "Epoch 00008: saving model to model_init_2021-03-2914_01_17.209734/model-00008-0.99817-0.61388-1.72685-0.28000.h5\n",
      "42/42 [==============================] - 144s 3s/step - loss: 0.9982 - categorical_accuracy: 0.6139 - val_loss: 1.7269 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8564 - categorical_accuracy: 0.6606\n",
      "Epoch 00009: saving model to model_init_2021-03-2914_01_17.209734/model-00009-0.85641-0.66063-1.54738-0.29000.h5\n",
      "42/42 [==============================] - 162s 4s/step - loss: 0.8564 - categorical_accuracy: 0.6606 - val_loss: 1.5474 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7700 - categorical_accuracy: 0.7119\n",
      "Epoch 00010: saving model to model_init_2021-03-2914_01_17.209734/model-00010-0.76997-0.71192-1.84321-0.27000.h5\n",
      "42/42 [==============================] - 171s 4s/step - loss: 0.7700 - categorical_accuracy: 0.7119 - val_loss: 1.8432 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f46d2ef14a8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modle 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_random_affine():\n",
    "    dx, dy = np.random.randint(-1.7, 1.8, 2)\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 120\n",
    "img_width = 120\n",
    "channels = 3\n",
    "img_idx = np.arange(0,30,1) #create a list of image numbers you want to use for a particular video\n",
    "frame_count = len(img_idx)\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "  \n",
    "            batch_data_aug2 = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug2 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    " \n",
    "            batch_data_aug3 = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug3 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                M2 = get_random_affine()\n",
    "                M3 = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "#                     height, width, channel = image.shape\n",
    "#                     mid_height = height//2\n",
    "#                     mid_width = width//2\n",
    "                    \n",
    "#                     resized = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60]\n",
    "                    # resized[:,:,0] = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60,0]\n",
    "                    # resized[:,:,1] = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60,1]\n",
    "                    # resized[:,:,2] = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60,2]\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes   \n",
    "                    resized = cv2.resize(image, (120,120), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug2[folder,idx] = cv2.warpAffine(resized, M2, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug3[folder,idx] = cv2.warpAffine(resized, M3, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug2[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug3[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug2, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug3, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug2, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug3, axis = 0)\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            \n",
    "            batch_data = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            \n",
    "            batch_data_aug = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "  \n",
    "            batch_data_aug2 = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug2 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "   \n",
    "            batch_data_aug3 = np.zeros((batch_size,frame_count,img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels_aug3 = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "      \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                M = get_random_affine()\n",
    "                M2 = get_random_affine()\n",
    "                M3 = get_random_affine()\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item], cv2.IMREAD_COLOR)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "#                     height, width, channel = image.shape\n",
    "#                     mid_height = height//2\n",
    "#                     mid_width = width//2\n",
    "                    \n",
    "#                     resized = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60]\n",
    "                    # resized[:,:,0] = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60,0]\n",
    "                    # resized[:,:,1] = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60,1]\n",
    "                    # resized[:,:,2] = image[mid_height-60:mid_height+60, mid_width-60:mid_width+60,2]\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes   \n",
    "                    resized = cv2.resize(image, (120,120), interpolation = cv2.INTER_AREA)\n",
    "                    batch_data[folder,idx] = resized\n",
    "                    batch_data_aug[folder,idx] = cv2.warpAffine(resized, M, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug2[folder,idx] = cv2.warpAffine(resized, M2, (resized.shape[0], resized.shape[1]))\n",
    "                    batch_data_aug3[folder,idx] = cv2.warpAffine(resized, M3, (resized.shape[0], resized.shape[1]))\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug2[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                batch_labels_aug3[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            batch_data = np.append(batch_data, batch_data_aug, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug2, axis = 0) \n",
    "            batch_data = np.append(batch_data, batch_data_aug3, axis = 0)\n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug2, axis = 0) \n",
    "            batch_labels = np.append(batch_labels, batch_labels_aug3, axis = 0) \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 20\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 20 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=(len(img_idx),img_height,img_width,channels), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 30, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 30, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 15, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 15, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 7, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 7, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 7, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 3, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 3, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"Adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0329 14:37:36.719548 140555769276224 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 16\n",
      "Epoch 1/20\n",
      "40/42 [===========================>..] - ETA: 10s - loss: 1.9770 - categorical_accuracy: 0.2285Batch:  42 Index: 16\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.9633 - categorical_accuracy: 0.2293Source path =  Project_data/val ; batch size = 16\n",
      "Batch:  7 Index: 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2914_37_17.500815/model-00001-1.96327-0.22926-1.59864-0.23000.h5\n",
      "42/42 [==============================] - 247s 6s/step - loss: 1.9633 - categorical_accuracy: 0.2293 - val_loss: 1.5986 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.5333 - categorical_accuracy: 0.3129\n",
      "Epoch 00002: saving model to model_init_2021-03-2914_37_17.500815/model-00002-1.53325-0.31293-1.61987-0.14286.h5\n",
      "42/42 [==============================] - 69s 2s/step - loss: 1.5333 - categorical_accuracy: 0.3129 - val_loss: 1.6199 - val_categorical_accuracy: 0.1429 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.3982 - categorical_accuracy: 0.3997\n",
      "Epoch 00003: saving model to model_init_2021-03-2914_37_17.500815/model-00003-1.39825-0.39966-1.53328-0.17857.h5\n",
      "42/42 [==============================] - 69s 2s/step - loss: 1.3982 - categorical_accuracy: 0.3997 - val_loss: 1.5333 - val_categorical_accuracy: 0.1786 - lr: 0.0010\n",
      "Epoch 4/20\n",
      " 9/42 [=====>........................] - ETA: 42s - loss: 1.4433 - categorical_accuracy: 0.3889Batch:  95 Index: 7\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.3670 - categorical_accuracy: 0.4489\n",
      "Epoch 00004: saving model to model_init_2021-03-2914_37_17.500815/model-00004-1.36698-0.44891-1.36111-0.50000.h5\n",
      "42/42 [==============================] - 56s 1s/step - loss: 1.3670 - categorical_accuracy: 0.4489 - val_loss: 1.3611 - val_categorical_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2643 - categorical_accuracy: 0.4607\n",
      "Epoch 00005: saving model to model_init_2021-03-2914_37_17.500815/model-00005-1.26427-0.46071-1.32020-0.64286.h5\n",
      "42/42 [==============================] - 51s 1s/step - loss: 1.2643 - categorical_accuracy: 0.4607 - val_loss: 1.3202 - val_categorical_accuracy: 0.6429 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0879 - categorical_accuracy: 0.5702\n",
      "Epoch 00006: saving model to model_init_2021-03-2914_37_17.500815/model-00006-1.08792-0.57024-1.41525-0.39286.h5\n",
      "42/42 [==============================] - 51s 1s/step - loss: 1.0879 - categorical_accuracy: 0.5702 - val_loss: 1.4153 - val_categorical_accuracy: 0.3929 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "16/42 [==========>...................] - ETA: 27s - loss: 1.1045 - categorical_accuracy: 0.5406Batch:  133 Index: 5\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0784 - categorical_accuracy: 0.5797\n",
      "Epoch 00007: saving model to model_init_2021-03-2914_37_17.500815/model-00007-1.07844-0.57969-1.65359-0.24107.h5\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.0784 - categorical_accuracy: 0.5797 - val_loss: 1.6536 - val_categorical_accuracy: 0.2411 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0760 - categorical_accuracy: 0.5853\n",
      "Epoch 00008: saving model to model_init_2021-03-2914_37_17.500815/model-00008-1.07599-0.58532-1.08597-0.60714.h5\n",
      "42/42 [==============================] - 34s 807ms/step - loss: 1.0760 - categorical_accuracy: 0.5853 - val_loss: 1.0860 - val_categorical_accuracy: 0.6071 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0980 - categorical_accuracy: 0.5853\n",
      "Epoch 00009: saving model to model_init_2021-03-2914_37_17.500815/model-00009-1.09801-0.58532-1.38934-0.35714.h5\n",
      "42/42 [==============================] - 33s 796ms/step - loss: 1.0980 - categorical_accuracy: 0.5853 - val_loss: 1.3893 - val_categorical_accuracy: 0.3571 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.1843 - categorical_accuracy: 0.5278\n",
      "Epoch 00010: saving model to model_init_2021-03-2914_37_17.500815/model-00010-1.18433-0.52778-1.33626-0.38393.h5\n",
      "42/42 [==============================] - 33s 796ms/step - loss: 1.1843 - categorical_accuracy: 0.5278 - val_loss: 1.3363 - val_categorical_accuracy: 0.3839 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.2273 - categorical_accuracy: 0.4702\n",
      "Epoch 00011: saving model to model_init_2021-03-2914_37_17.500815/model-00011-1.22727-0.47024-1.42477-0.45536.h5\n",
      "42/42 [==============================] - 33s 776ms/step - loss: 1.2273 - categorical_accuracy: 0.4702 - val_loss: 1.4248 - val_categorical_accuracy: 0.4554 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0603 - categorical_accuracy: 0.5794\n",
      "Epoch 00012: saving model to model_init_2021-03-2914_37_17.500815/model-00012-1.06028-0.57937-1.10837-0.78571.h5\n",
      "42/42 [==============================] - 34s 819ms/step - loss: 1.0603 - categorical_accuracy: 0.5794 - val_loss: 1.1084 - val_categorical_accuracy: 0.7857 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0363 - categorical_accuracy: 0.5615\n",
      "Epoch 00013: saving model to model_init_2021-03-2914_37_17.500815/model-00013-1.03632-0.56151-1.56487-0.28571.h5\n",
      "42/42 [==============================] - 33s 796ms/step - loss: 1.0363 - categorical_accuracy: 0.5615 - val_loss: 1.5649 - val_categorical_accuracy: 0.2857 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0226 - categorical_accuracy: 0.6210\n",
      "Epoch 00014: saving model to model_init_2021-03-2914_37_17.500815/model-00014-1.02262-0.62103-1.26318-0.44643.h5\n",
      "42/42 [==============================] - 34s 807ms/step - loss: 1.0226 - categorical_accuracy: 0.6210 - val_loss: 1.2632 - val_categorical_accuracy: 0.4464 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9262 - categorical_accuracy: 0.6230\n",
      "Epoch 00015: saving model to model_init_2021-03-2914_37_17.500815/model-00015-0.92617-0.62302-1.51707-0.35714.h5\n",
      "42/42 [==============================] - 33s 795ms/step - loss: 0.9262 - categorical_accuracy: 0.6230 - val_loss: 1.5171 - val_categorical_accuracy: 0.3571 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9874 - categorical_accuracy: 0.6111\n",
      "Epoch 00016: saving model to model_init_2021-03-2914_37_17.500815/model-00016-0.98737-0.61111-1.67741-0.32143.h5\n",
      "42/42 [==============================] - 33s 795ms/step - loss: 0.9874 - categorical_accuracy: 0.6111 - val_loss: 1.6774 - val_categorical_accuracy: 0.3214 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8289 - categorical_accuracy: 0.6687\n",
      "Epoch 00017: saving model to model_init_2021-03-2914_37_17.500815/model-00017-0.82893-0.66865-1.30533-0.39286.h5\n",
      "42/42 [==============================] - 33s 777ms/step - loss: 0.8289 - categorical_accuracy: 0.6687 - val_loss: 1.3053 - val_categorical_accuracy: 0.3929 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7358 - categorical_accuracy: 0.7421\n",
      "Epoch 00018: saving model to model_init_2021-03-2914_37_17.500815/model-00018-0.73576-0.74206-1.42642-0.35714.h5\n",
      "42/42 [==============================] - 33s 787ms/step - loss: 0.7358 - categorical_accuracy: 0.7421 - val_loss: 1.4264 - val_categorical_accuracy: 0.3571 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.0782 - categorical_accuracy: 0.6151\n",
      "Epoch 00019: saving model to model_init_2021-03-2914_37_17.500815/model-00019-1.07816-0.61508-1.30405-0.46429.h5\n",
      "42/42 [==============================] - 33s 788ms/step - loss: 1.0782 - categorical_accuracy: 0.6151 - val_loss: 1.3040 - val_categorical_accuracy: 0.4643 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.8946 - categorical_accuracy: 0.6310\n",
      "Epoch 00020: saving model to model_init_2021-03-2914_37_17.500815/model-00020-0.89462-0.63095-1.79857-0.25000.h5\n",
      "42/42 [==============================] - 35s 823ms/step - loss: 0.8946 - categorical_accuracy: 0.6310 - val_loss: 1.7986 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd50e5c4588>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 10\n",
    "\n",
    "conv3d without dropout\n",
    "in color image with size 80x120 but cropped images after resizing\n",
    "frames used - 6\n",
    "image is normalised to the (95-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = np.arange(10,21,2)\n",
    "    img_height = 160\n",
    "    img_width = 160\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    if image.height==360 and image.width==360:\n",
    "                        image = image.resize((160,160))\n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-80,mid_height-80,mid_width+80,mid_height+80))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                   \n",
    "                    if image.height==360 and image.width==360:\n",
    "                        image = image.resize((160,160))\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    \n",
    "                    image = image.crop((mid_width-80,mid_height-80,mid_width+80,mid_height+80))\n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "\n",
    "img_height = 160\n",
    "img_width = 160\n",
    "channels = 3\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same', input_shape=(6,img_height,img_width,channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 6, 160, 160, 32)   2624      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 6, 160, 160, 32)   0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 6, 160, 160, 32)   27680     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6, 160, 160, 32)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6, 160, 160, 32)   128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 2, 54, 54, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 2, 54, 54, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2, 54, 54, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 54, 54, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 54, 54, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2, 54, 54, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 1, 18, 18, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               10617344  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 2565      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 10,818,661\n",
      "Trainable params: 10,817,445\n",
      "Non-trainable params: 1,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = 'Adam'\n",
    "\n",
    "# compile it\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['categorical_accuracy'])\n",
    "\n",
    "# summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0328 06:18:09.168770 139986328037184 deprecation.py:323] From <ipython-input-11-bd77c9c60c14>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 32\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.1173 - categorical_accuracy: 0.4057Source path =  Project_data/val ; batch size = 32\n",
      "\n",
      "Epoch 00001: saving model to model_2021-03-2806_18_05.764754/model-00001-2.11729-0.40573-1.86506-0.25000.h5\n",
      "21/21 [==============================] - 48s 2s/step - loss: 2.1173 - categorical_accuracy: 0.4057 - val_loss: 1.8651 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8516 - categorical_accuracy: 0.6742\n",
      "Epoch 00002: saving model to model_2021-03-2806_18_05.764754/model-00002-0.85158-0.67421-2.41959-0.27000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.8516 - categorical_accuracy: 0.6742 - val_loss: 2.4196 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5370 - categorical_accuracy: 0.8069\n",
      "Epoch 00003: saving model to model_2021-03-2806_18_05.764754/model-00003-0.53699-0.80694-7.18828-0.22000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.5370 - categorical_accuracy: 0.8069 - val_loss: 7.1883 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3212 - categorical_accuracy: 0.8959\n",
      "Epoch 00004: saving model to model_2021-03-2806_18_05.764754/model-00004-0.32116-0.89593-9.36341-0.25000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.3212 - categorical_accuracy: 0.8959 - val_loss: 9.3634 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2567 - categorical_accuracy: 0.9155\n",
      "Epoch 00005: saving model to model_2021-03-2806_18_05.764754/model-00005-0.25665-0.91554-11.95388-0.22000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.2567 - categorical_accuracy: 0.9155 - val_loss: 11.9539 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1329 - categorical_accuracy: 0.9623\n",
      "Epoch 00006: saving model to model_2021-03-2806_18_05.764754/model-00006-0.13285-0.96229-15.46944-0.24000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.1329 - categorical_accuracy: 0.9623 - val_loss: 15.4694 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0773 - categorical_accuracy: 0.9834\n",
      "Epoch 00007: saving model to model_2021-03-2806_18_05.764754/model-00007-0.07733-0.98341-17.03047-0.21000.h5\n",
      "21/21 [==============================] - 43s 2s/step - loss: 0.0773 - categorical_accuracy: 0.9834 - val_loss: 17.0305 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0590 - categorical_accuracy: 0.9834\n",
      "Epoch 00008: saving model to model_2021-03-2806_18_05.764754/model-00008-0.05902-0.98341-19.67587-0.22000.h5\n",
      "21/21 [==============================] - 42s 2s/step - loss: 0.0590 - categorical_accuracy: 0.9834 - val_loss: 19.6759 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.0730 - categorical_accuracy: 0.9804\n",
      "Epoch 00009: saving model to model_2021-03-2806_18_05.764754/model-00009-0.07304-0.98039-22.74221-0.22000.h5\n",
      "21/21 [==============================] - 44s 2s/step - loss: 0.0730 - categorical_accuracy: 0.9804 - val_loss: 22.7422 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.1170 - categorical_accuracy: 0.9638\n",
      "Epoch 00010: saving model to model_2021-03-2806_18_05.764754/model-00010-0.11697-0.96380-15.52157-0.26000.h5\n",
      "21/21 [==============================] - 44s 2s/step - loss: 0.1170 - categorical_accuracy: 0.9638 - val_loss: 15.5216 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f50804a2748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 11 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel is used) is used <br>\n",
    "Image is first cropped to the standard size of 120x120<br>\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 ( the frame from 10 to 20) <br>\n",
    "Filter size - (3x3x3) <br>\n",
    "With 3 dropouts each of value 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 120\n",
    "img_width = 120\n",
    "channels = 3\n",
    "# img_idx = np.arange(10,21,2)\n",
    "img_idx = np.arange(10,20,1)\n",
    "\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    # image = image.resize((160,160))\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    # image = image.resize((160,160))\n",
    "                    \n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "# train_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/train'\n",
    "# val_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same', input_shape=(len(img_idx),img_height,img_width,channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu',kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# # model.add(Dense(64,activation='relu',kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dense(64,activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_8 (Conv3D)            (None, 10, 120, 120, 32)  2624      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "conv3d_9 (Conv3D)            (None, 10, 120, 120, 32)  27680     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 10, 120, 120, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3 (None, 4, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 4, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 4, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_11 (Conv3D)           (None, 4, 40, 40, 64)     110656    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 4, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3 (None, 2, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                1605696   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 1,802,981\n",
      "Trainable params: 1,802,661\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"Adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0328 12:36:38.387002 140095317186368 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 16\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.5796 - categorical_accuracy: 0.2534Source path =  Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2812_28_36.234442/model-00001-3.57962-0.25339-3.17095-0.25000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 3.5796 - categorical_accuracy: 0.2534 - val_loss: 3.1710 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.0591 - categorical_accuracy: 0.4133\n",
      "Epoch 00002: saving model to model_init_2021-03-2812_28_36.234442/model-00002-3.05909-0.41327-2.95739-0.37000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 3.0591 - categorical_accuracy: 0.4133 - val_loss: 2.9574 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.7578 - categorical_accuracy: 0.4419\n",
      "Epoch 00003: saving model to model_init_2021-03-2812_28_36.234442/model-00003-2.75783-0.44193-3.53797-0.30000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 2.7578 - categorical_accuracy: 0.4419 - val_loss: 3.5380 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.5309 - categorical_accuracy: 0.5309\n",
      "Epoch 00004: saving model to model_init_2021-03-2812_28_36.234442/model-00004-2.53087-0.53092-3.47905-0.28000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 2.5309 - categorical_accuracy: 0.5309 - val_loss: 3.4791 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.4567 - categorical_accuracy: 0.5641\n",
      "Epoch 00005: saving model to model_init_2021-03-2812_28_36.234442/model-00005-2.45669-0.56410-4.15611-0.23000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 2.4567 - categorical_accuracy: 0.5641 - val_loss: 4.1561 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.2474 - categorical_accuracy: 0.6425\n",
      "Epoch 00006: saving model to model_init_2021-03-2812_28_36.234442/model-00006-2.24737-0.64253-8.89534-0.21000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 2.2474 - categorical_accuracy: 0.6425 - val_loss: 8.8953 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.0653 - categorical_accuracy: 0.6998\n",
      "Epoch 00007: saving model to model_init_2021-03-2812_28_36.234442/model-00007-2.06527-0.69985-4.03745-0.26000.h5\n",
      "42/42 [==============================] - 43s 1s/step - loss: 2.0653 - categorical_accuracy: 0.6998 - val_loss: 4.0374 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.9338 - categorical_accuracy: 0.7255\n",
      "Epoch 00008: saving model to model_init_2021-03-2812_28_36.234442/model-00008-1.93382-0.72549-2.80782-0.42000.h5\n",
      "42/42 [==============================] - 43s 1s/step - loss: 1.9338 - categorical_accuracy: 0.7255 - val_loss: 2.8078 - val_categorical_accuracy: 0.4200 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.7321 - categorical_accuracy: 0.7828\n",
      "Epoch 00009: saving model to model_init_2021-03-2812_28_36.234442/model-00009-1.73210-0.78281-2.67640-0.40000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.7321 - categorical_accuracy: 0.7828 - val_loss: 2.6764 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6328 - categorical_accuracy: 0.8115\n",
      "Epoch 00010: saving model to model_init_2021-03-2812_28_36.234442/model-00010-1.63276-0.81146-2.54011-0.45000.h5\n",
      "42/42 [==============================] - 44s 1s/step - loss: 1.6328 - categorical_accuracy: 0.8115 - val_loss: 2.5401 - val_categorical_accuracy: 0.4500 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69d0463208>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 12 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel is used) is used <br>\n",
    "Image is first cropped to the standard size of 120x120<br>\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 ( the frame from 5 to 25) <br>\n",
    "Filter size - (3x3x3) <br>\n",
    "With 2 0.5 dropouts and 1 L2 regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 120\n",
    "img_width = 120\n",
    "channels = 3\n",
    "# img_idx = np.arange(10,21,2)\n",
    "img_idx = np.arange(5,25,1)\n",
    "\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    # image = image.resize((160,160))\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    # image = image.resize((160,160))\n",
    "                    \n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 10\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "# train_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/train'\n",
    "# val_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 10 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), padding='same', input_shape=(len(img_idx),img_height,img_width,channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(3, 3, 3),padding='same'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu',kernel_regularizer=l2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_14 (Conv3D)           (None, 20, 120, 120, 32)  2624      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 20, 120, 120, 32)  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 20, 120, 120, 32)  128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 7, 40, 40, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 7, 40, 40, 64)     55360     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 7, 40, 40, 64)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 7, 40, 40, 64)     256       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 3, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 37632)             0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                2408512   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 2,467,461\n",
      "Trainable params: 2,467,141\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"Adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0328 13:00:47.198309 140095317186368 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 16\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 3.5222 - categorical_accuracy: 0.3002Source path =  Project_data/val ; batch size = 16\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2813_00_46.057130/model-00001-3.52219-0.30015-5.63051-0.16000.h5\n",
      "42/42 [==============================] - 114s 3s/step - loss: 3.5222 - categorical_accuracy: 0.3002 - val_loss: 5.6305 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.9020 - categorical_accuracy: 0.4872\n",
      "Epoch 00002: saving model to model_init_2021-03-2813_00_46.057130/model-00002-2.90198-0.48718-18.33045-0.16000.h5\n",
      "42/42 [==============================] - 58s 1s/step - loss: 2.9020 - categorical_accuracy: 0.4872 - val_loss: 18.3305 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.4758 - categorical_accuracy: 0.6094\n",
      "Epoch 00003: saving model to model_init_2021-03-2813_00_46.057130/model-00003-2.47578-0.60935-22.50764-0.17000.h5\n",
      "42/42 [==============================] - 58s 1s/step - loss: 2.4758 - categorical_accuracy: 0.6094 - val_loss: 22.5076 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.2644 - categorical_accuracy: 0.6697\n",
      "Epoch 00004: saving model to model_init_2021-03-2813_00_46.057130/model-00004-2.26436-0.66968-26.32201-0.13000.h5\n",
      "42/42 [==============================] - 58s 1s/step - loss: 2.2644 - categorical_accuracy: 0.6697 - val_loss: 26.3220 - val_categorical_accuracy: 0.1300 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.0442 - categorical_accuracy: 0.7300\n",
      "Epoch 00005: saving model to model_init_2021-03-2813_00_46.057130/model-00005-2.04425-0.73002-21.73462-0.18000.h5\n",
      "42/42 [==============================] - 56s 1s/step - loss: 2.0442 - categorical_accuracy: 0.7300 - val_loss: 21.7346 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.9795 - categorical_accuracy: 0.7496\n",
      "Epoch 00006: saving model to model_init_2021-03-2813_00_46.057130/model-00006-1.97948-0.74962-15.33971-0.18000.h5\n",
      "42/42 [==============================] - 57s 1s/step - loss: 1.9795 - categorical_accuracy: 0.7496 - val_loss: 15.3397 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.8244 - categorical_accuracy: 0.7798\n",
      "Epoch 00007: saving model to model_init_2021-03-2813_00_46.057130/model-00007-1.82437-0.77979-26.39358-0.16000.h5\n",
      "42/42 [==============================] - 55s 1s/step - loss: 1.8244 - categorical_accuracy: 0.7798 - val_loss: 26.3936 - val_categorical_accuracy: 0.1600 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.7616 - categorical_accuracy: 0.8386\n",
      "Epoch 00008: saving model to model_init_2021-03-2813_00_46.057130/model-00008-1.76156-0.83861-19.49244-0.19000.h5\n",
      "42/42 [==============================] - 56s 1s/step - loss: 1.7616 - categorical_accuracy: 0.8386 - val_loss: 19.4924 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6320 - categorical_accuracy: 0.8582\n",
      "Epoch 00009: saving model to model_init_2021-03-2813_00_46.057130/model-00009-1.63198-0.85822-13.17592-0.20000.h5\n",
      "42/42 [==============================] - 56s 1s/step - loss: 1.6320 - categorical_accuracy: 0.8582 - val_loss: 13.1759 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.7280 - categorical_accuracy: 0.8205\n",
      "Epoch 00010: saving model to model_init_2021-03-2813_00_46.057130/model-00010-1.72801-0.82051-3.64872-0.27000.h5\n",
      "42/42 [==============================] - 55s 1s/step - loss: 1.7280 - categorical_accuracy: 0.8205 - val_loss: 3.6487 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69d04c16a0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 13 -\n",
    "Optimiser - Adam <br>\n",
    "RGB image (ie all the three channel is used) is used <br>\n",
    "Image is first cropped to the standard size of 120x120<br>\n",
    "image is min-max normalised to using 95 and 5 percentile instead of max and min <br>\n",
    "No of frames used per video = 10 ( the frame from 5 to 25) <br>\n",
    "Filter size - (3x3x3) <br>\n",
    "With 2 0.5 dropouts and 1 L2 regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 120\n",
    "img_width = 120\n",
    "channels = 3\n",
    "# img_idx = np.arange(10,21,2)\n",
    "img_idx = np.arange(5,25,1)\n",
    "\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list)//batch_size\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    # image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    # image = image.resize((160,160))\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    \n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        addnl_image_count = len(folder_list) % batch_size\n",
    "        batch = batch + 1\n",
    "        if(addnl_image_count!=0):\n",
    "            batch_data = np.zeros((addnl_image_count,len(img_idx),img_height,img_width,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((addnl_image_count,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(addnl_image_count): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = Image.open(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item])\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "                    mid_height = image.height//2\n",
    "                    mid_width = image.width//2\n",
    "                    image = image.crop((mid_width-60,mid_height-60,mid_width+60,mid_height+60))\n",
    "                    # image = image.resize((160,160))\n",
    "                    \n",
    "                    image = (image - np.percentile(image,5))/ (np.percentile(image,95) - np.percentile(image,5))\n",
    "                    batch_data[folder,idx,:,:,0] = image[:,:,0]\n",
    "                    batch_data[folder,idx,:,:,1] = image[:,:,1]\n",
    "                    batch_data[folder,idx,:,:,2] = image[:,:,2]\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 25\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "# train_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/train'\n",
    "# val_path = 'gdrive/MyDrive/Colab Notebooks/Gesture Recognition/Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 25 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)\n",
    "num_classes = 5\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv3D, MaxPooling3D, AveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(8, kernel_size=(3,3,3), input_shape=(len(img_idx),img_height,img_width,channels), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(16, kernel_size=(3,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(1,3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "#Flatten Layers\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#softmax layer\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 20, 120, 120, 8)   656       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 20, 120, 120, 8)   32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 20, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 120, 120, 8)   0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 10, 60, 60, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 10, 60, 60, 16)    3472      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 60, 60, 16)    64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 60, 60, 16)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 5, 30, 30, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 5, 30, 30, 32)     4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 5, 30, 30, 32)     128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 30, 30, 32)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 2, 15, 15, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 2, 15, 15, 64)     18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 15, 15, 64)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 1, 7, 7, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 864,101\n",
      "Trainable params: 863,989\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = \"Adam\"\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0328 14:09:24.889477 140539319473984 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.01)\n",
    "callbacks_list = [checkpoint, LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 64\n",
      "Epoch 1/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 2.9180 - categorical_accuracy: 0.2036Source path =  Project_data/val ; batch size = 64\n",
      "\n",
      "Epoch 00001: saving model to model_init_2021-03-2814_09_04.469840/model-00001-2.91797-0.20362-1.60787-0.25000.h5\n",
      "11/11 [==============================] - 122s 11s/step - loss: 2.9180 - categorical_accuracy: 0.2036 - val_loss: 1.6079 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.6142 - categorical_accuracy: 0.2293\n",
      "Epoch 00002: saving model to model_init_2021-03-2814_09_04.469840/model-00002-1.61418-0.22926-1.60825-0.22000.h5\n",
      "11/11 [==============================] - 57s 5s/step - loss: 1.6142 - categorical_accuracy: 0.2293 - val_loss: 1.6082 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.5919 - categorical_accuracy: 0.2428\n",
      "Epoch 00003: saving model to model_init_2021-03-2814_09_04.469840/model-00003-1.59195-0.24284-1.60901-0.17000.h5\n",
      "11/11 [==============================] - 59s 5s/step - loss: 1.5919 - categorical_accuracy: 0.2428 - val_loss: 1.6090 - val_categorical_accuracy: 0.1700 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.5693 - categorical_accuracy: 0.2594\n",
      "Epoch 00004: saving model to model_init_2021-03-2814_09_04.469840/model-00004-1.56934-0.25943-1.60855-0.19000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 1.5693 - categorical_accuracy: 0.2594 - val_loss: 1.6086 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.5329 - categorical_accuracy: 0.3484\n",
      "Epoch 00005: saving model to model_init_2021-03-2814_09_04.469840/model-00005-1.53291-0.34842-1.60705-0.27000.h5\n",
      "11/11 [==============================] - 59s 5s/step - loss: 1.5329 - categorical_accuracy: 0.3484 - val_loss: 1.6071 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.4664 - categorical_accuracy: 0.3544\n",
      "Epoch 00006: saving model to model_init_2021-03-2814_09_04.469840/model-00006-1.46643-0.35445-1.59981-0.30000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 1.4664 - categorical_accuracy: 0.3544 - val_loss: 1.5998 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.3662 - categorical_accuracy: 0.4359\n",
      "Epoch 00007: saving model to model_init_2021-03-2814_09_04.469840/model-00007-1.36618-0.43590-1.59332-0.28000.h5\n",
      "11/11 [==============================] - 57s 5s/step - loss: 1.3662 - categorical_accuracy: 0.4359 - val_loss: 1.5933 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2865 - categorical_accuracy: 0.4736\n",
      "Epoch 00008: saving model to model_init_2021-03-2814_09_04.469840/model-00008-1.28649-0.47360-1.59558-0.28000.h5\n",
      "11/11 [==============================] - 55s 5s/step - loss: 1.2865 - categorical_accuracy: 0.4736 - val_loss: 1.5956 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.2192 - categorical_accuracy: 0.4977\n",
      "Epoch 00009: saving model to model_init_2021-03-2814_09_04.469840/model-00009-1.21918-0.49774-1.57667-0.27000.h5\n",
      "11/11 [==============================] - 57s 5s/step - loss: 1.2192 - categorical_accuracy: 0.4977 - val_loss: 1.5767 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.1883 - categorical_accuracy: 0.5234\n",
      "Epoch 00010: saving model to model_init_2021-03-2814_09_04.469840/model-00010-1.18826-0.52338-1.56696-0.23000.h5\n",
      "11/11 [==============================] - 54s 5s/step - loss: 1.1883 - categorical_accuracy: 0.5234 - val_loss: 1.5670 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0861 - categorical_accuracy: 0.5656\n",
      "Epoch 00011: saving model to model_init_2021-03-2814_09_04.469840/model-00011-1.08610-0.56561-1.59519-0.19000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 1.0861 - categorical_accuracy: 0.5656 - val_loss: 1.5952 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 1.0269 - categorical_accuracy: 0.5837\n",
      "Epoch 00012: saving model to model_init_2021-03-2814_09_04.469840/model-00012-1.02690-0.58371-1.58547-0.34000.h5\n",
      "11/11 [==============================] - 53s 5s/step - loss: 1.0269 - categorical_accuracy: 0.5837 - val_loss: 1.5855 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9285 - categorical_accuracy: 0.6501\n",
      "Epoch 00013: saving model to model_init_2021-03-2814_09_04.469840/model-00013-0.92855-0.65008-1.59839-0.32000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 0.9285 - categorical_accuracy: 0.6501 - val_loss: 1.5984 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.9491 - categorical_accuracy: 0.6350\n",
      "Epoch 00014: saving model to model_init_2021-03-2814_09_04.469840/model-00014-0.94906-0.63499-1.55629-0.26000.h5\n",
      "11/11 [==============================] - 54s 5s/step - loss: 0.9491 - categorical_accuracy: 0.6350 - val_loss: 1.5563 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.8655 - categorical_accuracy: 0.6652\n",
      "Epoch 00015: saving model to model_init_2021-03-2814_09_04.469840/model-00015-0.86552-0.66516-1.56002-0.30000.h5\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.8655 - categorical_accuracy: 0.6652 - val_loss: 1.5600 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7758 - categorical_accuracy: 0.7149\n",
      "Epoch 00016: saving model to model_init_2021-03-2814_09_04.469840/model-00016-0.77578-0.71493-1.64135-0.19000.h5\n",
      "11/11 [==============================] - 54s 5s/step - loss: 0.7758 - categorical_accuracy: 0.7149 - val_loss: 1.6414 - val_categorical_accuracy: 0.1900 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.7102 - categorical_accuracy: 0.7557\n",
      "Epoch 00017: saving model to model_init_2021-03-2814_09_04.469840/model-00017-0.71020-0.75566-1.66420-0.21000.h5\n",
      "11/11 [==============================] - 57s 5s/step - loss: 0.7102 - categorical_accuracy: 0.7557 - val_loss: 1.6642 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.6562 - categorical_accuracy: 0.7587\n",
      "Epoch 00018: saving model to model_init_2021-03-2814_09_04.469840/model-00018-0.65623-0.75867-1.69108-0.28000.h5\n",
      "11/11 [==============================] - 53s 5s/step - loss: 0.6562 - categorical_accuracy: 0.7587 - val_loss: 1.6911 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5477 - categorical_accuracy: 0.7979\n",
      "Epoch 00019: saving model to model_init_2021-03-2814_09_04.469840/model-00019-0.54770-0.79789-1.62911-0.30000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 0.5477 - categorical_accuracy: 0.7979 - val_loss: 1.6291 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.5088 - categorical_accuracy: 0.8145\n",
      "Epoch 00020: saving model to model_init_2021-03-2814_09_04.469840/model-00020-0.50881-0.81448-1.64195-0.31000.h5\n",
      "11/11 [==============================] - 54s 5s/step - loss: 0.5088 - categorical_accuracy: 0.8145 - val_loss: 1.6419 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4512 - categorical_accuracy: 0.8356\n",
      "Epoch 00021: saving model to model_init_2021-03-2814_09_04.469840/model-00021-0.45116-0.83560-1.66496-0.27000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 0.4512 - categorical_accuracy: 0.8356 - val_loss: 1.6650 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.4431 - categorical_accuracy: 0.8235\n",
      "Epoch 00022: saving model to model_init_2021-03-2814_09_04.469840/model-00022-0.44306-0.82353-1.73488-0.30000.h5\n",
      "11/11 [==============================] - 54s 5s/step - loss: 0.4431 - categorical_accuracy: 0.8235 - val_loss: 1.7349 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n",
      "Epoch 23/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3503 - categorical_accuracy: 0.8839\n",
      "Epoch 00023: saving model to model_init_2021-03-2814_09_04.469840/model-00023-0.35028-0.88386-1.72763-0.31000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 0.3503 - categorical_accuracy: 0.8839 - val_loss: 1.7276 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
      "Epoch 24/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3511 - categorical_accuracy: 0.8537\n",
      "Epoch 00024: saving model to model_init_2021-03-2814_09_04.469840/model-00024-0.35105-0.85370-1.64928-0.28000.h5\n",
      "11/11 [==============================] - 53s 5s/step - loss: 0.3511 - categorical_accuracy: 0.8537 - val_loss: 1.6493 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
      "Epoch 25/25\n",
      "11/11 [==============================] - ETA: 0s - loss: 0.3194 - categorical_accuracy: 0.8869\n",
      "Epoch 00025: saving model to model_init_2021-03-2814_09_04.469840/model-00025-0.31942-0.88688-1.74837-0.30000.h5\n",
      "11/11 [==============================] - 56s 5s/step - loss: 0.3194 - categorical_accuracy: 0.8869 - val_loss: 1.7484 - val_categorical_accuracy: 0.3000 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1380aaeb8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
