{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0af609e1-23c4-4811-afbc-399196ca22c1",
    "_uuid": "8b2af43c-2f26-4ad4-8ff1-bf1424e6f53c"
   },
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "430db985-a8ef-46db-adc0-115ee495c14c",
    "_uuid": "65f04bd4-f3bf-4b32-9a7f-a33e3ab3c227",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "31da836f-c8a6-4f2d-a046-856463dffb8d",
    "_uuid": "43ef0864-3825-4a33-808d-70345c0bda08"
   },
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "c5f0eecb-d84f-4fea-adfc-57486f2fa71c",
    "_uuid": "4e05ca6a-d7c9-48bb-b3f3-f09aa6c4a7a9",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfccc48e-37bf-4dba-857a-b4fd2c5fe85c",
    "_uuid": "0f1e12b3-7abe-4748-8dce-aca48571214a"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "7e16145c-0b41-408f-bb14-1524303d8558",
    "_uuid": "af3121ce-4fe9-4b71-acc7-a541b9ac43fd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Q_dict = collections.defaultdict(dict)\n",
    "# States_track = collections.defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To initialise the track states\n",
    "# def initialise_tracking_states():\n",
    "#     # Tracking the state action pair ((2,2,0), (0,0)). \n",
    "#     # Since the action (0,0) is at position 0, we will denote action index 0 to point to 0 action\n",
    "#     sample_q_values = [(\"0_1_0_0_0_0_0_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_1_0_0_0_0_0_0\", 0)]\n",
    "#     for q_value in sample_q_values:\n",
    "#         state = q_value[0]\n",
    "#         action = q_value[1]\n",
    "#         States_track[state][action] = []\n",
    "\n",
    "# # # Saving the q values of the track states\n",
    "# def save_tracking_states():\n",
    "#     for state in States_track.keys():\n",
    "#         for action_index in States_track[state].keys():\n",
    "#             q_value = agent.model.predict(np.array(state.split(\"_\")).reshape(1,36))\n",
    "#             States_track[state][action].append(q_value[0][action_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b8105f25-0afc-4df1-af0a-7b94a3b7f925",
    "_uuid": "0e191b15-cf0c-4150-810a-ddee9faeb68f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Saving the rewards per episode in the pickle file\n",
    "\n",
    "def save_pickle(obj, name):\n",
    "    write_start_time = time.time()\n",
    "    with open(name + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    write_time_taken = time.time()-write_start_time\n",
    "    print(\"Writing the pickle file completed in \" + str(write_time_taken/60) + \" minutes\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "07546f78-752d-44b8-9b32-e7f5fdd3eaba",
    "_uuid": "eb07d54d-97f0-4349-98af-af36530201ed"
   },
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "a8f3d234-9287-4863-a758-dd6329d40f2f",
    "_uuid": "5a62105e-c977-47c4-ac2e-6a17eb6dded2",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01     \n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = -0.0005\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.state_to_track = (2,2,0)\n",
    "        self.action_index_to_track = 0\n",
    "        self.state_tracking = []\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets     \n",
    "        \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(32,activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size,activation=\"relu\"))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment       \n",
    "#         epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-0.000001*episode)\n",
    "        z = np.random.random()\n",
    "        \n",
    "        possible_actions_index, agent_allowable_actions = list(env.requests(state))        \n",
    "        if z > self.epsilon:\n",
    "            # Exploitation\n",
    "            encoded_state =np.array(env.state_encod_arch1(state)).reshape(1,36)\n",
    "            q_value = self.model.predict(encoded_state)    \n",
    "            allowed_q_values = [q_value[0][i] for i in possible_actions_index]\n",
    "            action_index = possible_actions_index[np.argmax(allowed_q_values)]\n",
    "        else:\n",
    "            # Exploration\n",
    "            action_index = random.choice(possible_actions_index)            \n",
    "        return action_index\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, terminal_state):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action_index, reward, next_state, terminal_state))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, terminal_state = mini_batch[i]\n",
    "                \n",
    "                actions.append(action_index)\n",
    "                rewards.append(reward)\n",
    "                done.append(terminal_state)\n",
    "                \n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                \n",
    "            # Write your code from here\n",
    "            # 1. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)   \n",
    "\n",
    "            # 2. Get the target for the Q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])  \n",
    "                \n",
    "                \n",
    "            # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "    def save(self, name):\n",
    "        save_start_time = time.time()\n",
    "        self.model.save(name)\n",
    "        save_time_taken = time.time() - save_start_time\n",
    "        print(\"Saving the model completed in \" + str(save_time_taken/60) + \" minu\")\n",
    "\n",
    "    # # Saving the q values of the track states\n",
    "    def save_tracking_states(self):\n",
    "        q_value = agent.model.predict(np.array(env.state_encod_arch1(self.state_to_track)).reshape(1,36))\n",
    "        self.state_tracking.append(q_value[0][self.action_index_to_track])                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "f01cd0cf-aa9f-4499-b55f-603506c6830a",
    "_uuid": "f328c6c3-2970-4436-af71-59e4c4812ebb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Episodes = 15000\n",
    "write_threshold = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0c6fe4d-9d18-4454-aa76-e4b1e2c1ca7a",
    "_uuid": "20ef9e64-29ff-4645-b8dc-762271f90e0f"
   },
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "63882afa-87d8-4677-aa06-2503d86d0ec9",
    "_uuid": "0377131a-bc92-4e67-a955-5a857511ec88",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-849bc7ae206e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mcount\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# 1. Pick epsilon-greedy action from possible actions for the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0maction_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# 2. Evaluate your reward and next state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-09531b8181ff>\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, state, episode)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mpossible_actions_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_allowable_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m# Exploitation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIML Pythn Notebooks\\Reinforcement Learning\\Deep Reinforcement Learning\\Assignment\\RL Project(Cab-Driver)-Code Structure\\Env.py\u001b[0m in \u001b[0;36mrequests\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \"\"\"Determining the number of requests basis the location. \n\u001b[0;32m     59\u001b[0m         Use the table specified in the MDP and complete for rest of the locations\"\"\"\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlocation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mrequests\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoisson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "state_size = 36\n",
    "action_size = 21\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    score = 0\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    env = CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    (action_space, state_space, state) = env.reset()\n",
    "    terminal_state = False\n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    count = 0\n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        count+=1\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action_index = agent.get_action(state, episode)\n",
    "        action = action_space[action_index]\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward = env.reward_func(state, action, Time_matrix)\n",
    "        next_state, terminal_state = env.next_state_func(state, action, Time_matrix) \n",
    "        # 3. Append the experience to the memory\n",
    "        agent.append_sample(state, action_index, reward, next_state, terminal_state)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        if count%20 == 0:\n",
    "            agent.train_model()\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        score += reward\n",
    "        state = next_state\n",
    "    \n",
    "    # Store total rewards obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "    # epsilon decay\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "#         agent.epsilon *= agent.epsilon_decay\n",
    "        agent.epsilon = agent.epsilon_min + (agent.epsilon_max - agent.epsilon_min) * np.exp(agent.epsilon_decay*episode)\n",
    "    \n",
    "    # Logging every episode\n",
    "#     print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}\".format(episode, score, len(agent.memory), agent.epsilon))\n",
    "    \n",
    "    # Initialising the State_track dictionary to track the state\n",
    "#     if (episode == track_threshold-1):\n",
    "#         initialise_tracking_states()\n",
    "\n",
    "    if (episode%500 ==0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}\".format(episode, score, len(agent.memory), agent.epsilon))        \n",
    "    \n",
    "    if (episode%10 ==0):\n",
    "        agent.save_tracking_states()\n",
    "\n",
    "    # Every few episodes\n",
    "    if episode%write_threshold == 0:\n",
    "        # Saving the models\n",
    "        agent.save(\"model_weights.h5\")\n",
    "        # Saving the rewards per episode as a pickle file\n",
    "        save_pickle(rewards_per_episode,\"rewards_per_episode\")\n",
    "\n",
    "##### Simulation ends ######\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time/3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57796a49-f5fa-41dc-9394-ae7a22eb3ef7",
    "_uuid": "609d36a4-f07d-4eb9-b986-3764adbad99f"
   },
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb11f917-1b54-42b4-93fc-eaa0b684987b",
    "_uuid": "4ad8c490-1008-410f-8561-3c2d14ddd6a1",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"rewards_per_episode.pkl\", \"rb\") as f:\n",
    "    rewards_per_episode = pickle.load(f)\n",
    "\n",
    "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
    "plt.xlabel(\"episode number\")\n",
    "plt.ylabel(\"rewards per episode\")\n",
    "plt.savefig('rewards.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.remove(\"./model_weights.h5\")\n",
    "# os.remove(\"./rewards_per_episode.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3dd2aa28-35ec-470b-a1be-ce6e4e3b7551",
    "_uuid": "b6301198-c23a-425c-9715-8bc2557b6a35"
   },
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04efc647-9226-41ee-ab2d-b4ac4ee5e569",
    "_uuid": "bba09de4-e91e-4f13-add9-9bb89d260490"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = -0.0005\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.epsilon = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f1274125-5c80-4720-84aa-015b6d4f1c40",
    "_uuid": "8244329d-3737-46d9-a226-823abd83f7ac",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "time = np.arange(0,15000)\n",
    "epsilon = []\n",
    "for i in range(0,15000):\n",
    "    epsilon.append(0.00001 + (1 - 0.00001) * np.exp(-0.0005*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8389caa-942c-4f35-a4e7-0bae53f53787",
    "_uuid": "7da80223-bf90-4a30-91da-5a312de5d932",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69407602-ca5f-4671-9ec3-5123e9d22125",
    "_uuid": "92a7fa52-93f3-4793-870a-06d2967ec631",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
