{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import routines\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Defining hyperparameters\n",
    "m = 5 # number of cities, ranges from 1 ..... m\n",
    "t = 24 # number of hours, ranges from 0 .... t-1\n",
    "d = 7  # number of days, ranges from 0 ... d-1\n",
    "C = 5 # Per hour fuel and other costs\n",
    "R = 9 # per hour revenue from a passenger\n",
    "\n",
    "\n",
    "class CabDriver():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        \"\"\"initialise your state and define your action space and state space\"\"\"\n",
    "        self.action_space = [(0,0)] + [(p,q) for p in range(1, m+1) for q in range(1, m+1) if p!=q or p==0]\n",
    "        # Location ranges from 1 to 5 and is not starting from 0\n",
    "        self.state_space = [(loc, time, day) for loc in range(1, m+1) for time in range(t) for day in range(d)]\n",
    "        self.state_init = random.choice(self.state_space)\n",
    "        \n",
    "        self.time_elapsed = 0 # To track if the cab driver has completed 30 days\n",
    "\n",
    "        # Start the first round\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    ## Encoding state (or state-action) for NN input\n",
    "\n",
    "    def state_encod_arch1(self, state):\n",
    "        \"\"\"convert the state into a vector so that it can be fed to the NN. This method converts a given state into a vector format. Hint: The vector is of size m + t + d.\"\"\"\n",
    "        \n",
    "        state_encod = [0 for i in range(m+t+d)] # Creating a encoded state vector of all zeros\n",
    "        state_encod[state[0] -1] = 1 # Changing the value to 1 to encode location info\n",
    "        state_encod[m + state[1]] = 1 # Changing the value to 1 encode time information\n",
    "        state_encod[m + t + state[2]] = 1 # Changing the value to 1 encode date information\n",
    "        \n",
    "        return state_encod\n",
    "\n",
    "\n",
    "    # Use this function if you are using architecture-2 \n",
    "    # def state_encod_arch2(self, state, action):\n",
    "    #     \"\"\"convert the (state-action) into a vector so that it can be fed to the NN. This method converts a given state-action pair into a vector format. Hint: The vector is of size m + t + d + m + m.\"\"\"\n",
    "\n",
    "        \n",
    "    #     return state_encod\n",
    "\n",
    "\n",
    "    ## Getting number of requests\n",
    "\n",
    "    def requests(self, state):\n",
    "        \"\"\"Determining the number of requests basis the location. \n",
    "        Use the table specified in the MDP and complete for rest of the locations\"\"\"\n",
    "        location = state[0]\n",
    "#         if location == 0:\n",
    "#             requests = np.random.poisson(2)\n",
    "\n",
    "        if location == 1:\n",
    "            requests = np.random.poisson(2)\n",
    "        if location == 2:\n",
    "            requests = np.random.poisson(12)\n",
    "        if location == 3:\n",
    "            requests = np.random.poisson(4)\n",
    "        if location == 4:\n",
    "            requests = np.random.poisson(7)\n",
    "        if location == 5:\n",
    "            requests = np.random.poisson(8)\n",
    "\n",
    "        if requests >15:\n",
    "            requests =15\n",
    "\n",
    "        possible_actions_index = random.sample(range(1, (m-1)*m +1), requests) # (0,0) is not considered as customer request\n",
    "        actions = [self.action_space[i] for i in possible_actions_index]\n",
    "\n",
    "        \n",
    "        actions.append([0,0])\n",
    "        possible_actions_index.append(0) # Appending the action index for the action (0,0)\n",
    "        return possible_actions_index,actions   \n",
    "\n",
    "\n",
    "\n",
    "    def reward_func(self, state, action, Time_matrix):\n",
    "        \"\"\"Takes in state, action and Time-matrix and returns the reward\"\"\"\n",
    "        \n",
    "        total_time   = 0\n",
    "        # Time taken for the cab driver to arriver to arrive at the pickup location from the current location\n",
    "        t1 = 0\n",
    "        # Actual time taken for the ride - from pickup to drop\n",
    "        t2 = 0\n",
    "        # Time spent when the cab driver goes offline\n",
    "        offline_time = 0    \n",
    "        \n",
    "        curr_loc = state[0]\n",
    "        curr_time = state[1]\n",
    "        curr_day = state[2]\n",
    "        pickup_loc = action[0]\n",
    "        drop_loc = action[1]\n",
    "        \n",
    "        # When option to go offline is selected\n",
    "        if ((pickup_loc== 0) and (drop_loc == 0)):\n",
    "            offline_time = 1\n",
    "        # when driver is already at the pickup location\n",
    "        elif (curr_loc == pickup_loc):\n",
    "            t2 = Time_matrix[pickup_loc-1][drop_loc-1][curr_time][curr_day]\n",
    "        # Driver is at a location different from the pickup location\n",
    "        else:\n",
    "            t1 = Time_matrix[curr_loc-1][pickup_loc-1][curr_time][curr_day]\n",
    "            time_at_pickup, day_at_pickup = self.update_time_day(curr_time, curr_day, t1)\n",
    "            t2 = Time_matrix[pickup_loc-1][drop_loc-1][time_at_pickup][day_at_pickup]\n",
    "\n",
    "        # Calculate total time as sum of all durations        \n",
    "        reward = (R * t2) - (C * (t2 + t1 + offline_time))\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def next_state_func(self, state, action, Time_matrix):\n",
    "        \"\"\"Takes state and action as input and returns next state\"\"\"\n",
    "        \n",
    "        next_state = []\n",
    "        # To indicate if the 30 hours has been completed\n",
    "        is_terminal = False\n",
    "        \n",
    "        # Initialize various times\n",
    "        total_time   = 0\n",
    "        # Time taken for the cab driver to arriver to get to the pickup location from the current location\n",
    "        t1 = 0\n",
    "        # Actual time taken for the ride - from pickup to drop\n",
    "        t2 = 0\n",
    "        # Time spent when the cab driver goes offline\n",
    "        offline_time = 0    \n",
    "        \n",
    "        curr_loc = state[0]\n",
    "        curr_time = state[1]\n",
    "        curr_day = state[2]\n",
    "        pickup_loc = action[0]\n",
    "        drop_loc = action[1]\n",
    "        \n",
    "        # When option to go offline is selected\n",
    "        if ((pickup_loc== 0) and (drop_loc == 0)):\n",
    "            offline_time = 1\n",
    "            next_loc = curr_loc\n",
    "        # when driver is already at the pickup location\n",
    "        elif (curr_loc == pickup_loc):\n",
    "            t2 = Time_matrix[pickup_loc-1][drop_loc-1][curr_time][curr_day]\n",
    "            next_loc = drop_loc\n",
    "        # Driver is at a location different from the pickup location\n",
    "        else:\n",
    "            t1 = Time_matrix[curr_loc-1][pickup_loc-1][curr_time][curr_day]\n",
    "            time_at_pickup, day_at_pickup = self.update_time_day(curr_time, curr_day, t1)\n",
    "            t2 = Time_matrix[pickup_loc-1][drop_loc-1][time_at_pickup][day_at_pickup]\n",
    "            next_loc  = drop_loc\n",
    "\n",
    "        # Calculate total time as sum of all durations\n",
    "        total_time = offline_time + t1 + t2\n",
    "        next_time, next_day = self.update_time_day(curr_time, curr_day, total_time)\n",
    "        self.time_elapsed += total_time\n",
    "        if self.time_elapsed >=720:\n",
    "            is_terminal = True\n",
    "        next_state = [next_loc, next_time, next_day]\n",
    "        return next_state, is_terminal\n",
    "\n",
    "    def update_time_day(self, time, day, ride_duration):\n",
    "        \"\"\"\n",
    "        Takes in the current state and time taken for driver's journey to return\n",
    "        the state post that journey.\n",
    "        \"\"\"\n",
    "        ride_duration = int(ride_duration)\n",
    "\n",
    "        if (time + ride_duration) < 24:\n",
    "            time = time + ride_duration\n",
    "            # day is unchanged\n",
    "        else:\n",
    "            # duration taken spreads over to subsequent days\n",
    "            # Get the number of days\n",
    "            num_days = (time + ride_duration) // 24\n",
    "            \n",
    "            # convert the time to 0-23 range\n",
    "            time = (time + ride_duration) % 24             \n",
    "            \n",
    "            # Convert the day to 0-6 range\n",
    "            day = (day + num_days ) % 7\n",
    "\n",
    "        return time, day\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.action_space, self.state_space, self.state_init\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0af609e1-23c4-4811-afbc-399196ca22c1",
    "_uuid": "8b2af43c-2f26-4ad4-8ff1-bf1424e6f53c"
   },
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "430db985-a8ef-46db-adc0-115ee495c14c",
    "_uuid": "65f04bd4-f3bf-4b32-9a7f-a33e3ab3c227",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Import the environment\n",
    "# from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "31da836f-c8a6-4f2d-a046-856463dffb8d",
    "_uuid": "43ef0864-3825-4a33-808d-70345c0bda08"
   },
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c5f0eecb-d84f-4fea-adfc-57486f2fa71c",
    "_uuid": "4e05ca6a-d7c9-48bb-b3f3-f09aa6c4a7a9",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "# Time_matrix = np.load(\"TM.npy\")\n",
    "Time_matrix = np.load(\"../input/time-matrix/TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bfccc48e-37bf-4dba-857a-b4fd2c5fe85c",
    "_uuid": "0f1e12b3-7abe-4748-8dce-aca48571214a"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "7e16145c-0b41-408f-bb14-1524303d8558",
    "_uuid": "af3121ce-4fe9-4b71-acc7-a541b9ac43fd",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Q_dict = collections.defaultdict(dict)\n",
    "# States_track = collections.defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To initialise the track states\n",
    "# def initialise_tracking_states():\n",
    "#     # Tracking the state action pair ((2,2,0), (0,0)). \n",
    "#     # Since the action (0,0) is at position 0, we will denote action index 0 to point to 0 action\n",
    "#     sample_q_values = [(\"0_1_0_0_0_0_0_1_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_0_1_0_0_0_0_0_0\", 0)]\n",
    "#     for q_value in sample_q_values:\n",
    "#         state = q_value[0]\n",
    "#         action = q_value[1]\n",
    "#         States_track[state][action] = []\n",
    "\n",
    "# # # Saving the q values of the track states\n",
    "# def save_tracking_states():\n",
    "#     for state in States_track.keys():\n",
    "#         for action_index in States_track[state].keys():\n",
    "#             q_value = agent.model.predict(np.array(state.split(\"_\")).reshape(1,36))\n",
    "#             States_track[state][action].append(q_value[0][action_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b8105f25-0afc-4df1-af0a-7b94a3b7f925",
    "_uuid": "0e191b15-cf0c-4150-810a-ddee9faeb68f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Saving the rewards per episode in the pickle file\n",
    "\n",
    "def save_pickle(obj, name):\n",
    "    write_start_time = time.time()\n",
    "    with open(name + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    write_time_taken = time.time()-write_start_time\n",
    "    print(\"Writing the pickle file completed in \" + str(write_time_taken/60) + \" minutes\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "07546f78-752d-44b8-9b32-e7f5fdd3eaba",
    "_uuid": "eb07d54d-97f0-4349-98af-af36530201ed"
   },
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "a8f3d234-9287-4863-a758-dd6329d40f2f",
    "_uuid": "5a62105e-c977-47c4-ac2e-6a17eb6dded2",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01     \n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = -0.0005\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.epsilon = 1.0\n",
    "\n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.state_to_track = (2,2,0)\n",
    "        self.action_index_to_track = 0\n",
    "        self.state_tracking = []\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets     \n",
    "        \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(32,activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size,activation=\"relu\"))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in ε after we generate each sample from the environment       \n",
    "#         epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-0.000001*episode)\n",
    "        z = np.random.random()\n",
    "        \n",
    "        possible_actions_index, agent_allowable_actions = list(env.requests(state))        \n",
    "        if z > self.epsilon:\n",
    "            # Exploitation\n",
    "            encoded_state =np.array(env.state_encod_arch1(state)).reshape(1,36)\n",
    "            q_value = self.model.predict(encoded_state)    \n",
    "            allowed_q_values = [q_value[0][i] for i in possible_actions_index]\n",
    "            action_index = possible_actions_index[np.argmax(allowed_q_values)]\n",
    "        else:\n",
    "            # Exploration\n",
    "            action_index = random.choice(possible_actions_index)            \n",
    "        return action_index\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, terminal_state):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action_index, reward, next_state, terminal_state))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, terminal_state = mini_batch[i]\n",
    "                \n",
    "                actions.append(action_index)\n",
    "                rewards.append(reward)\n",
    "                done.append(terminal_state)\n",
    "                \n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                \n",
    "            # Write your code from here\n",
    "            # 1. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)   \n",
    "\n",
    "            # 2. Get the target for the Q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "            #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])  \n",
    "                \n",
    "                \n",
    "            # 4. Fit your model and track the loss values\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "    def save(self, name):\n",
    "        save_start_time = time.time()\n",
    "        self.model.save(name)\n",
    "        save_time_taken = time.time() - save_start_time\n",
    "        print(\"Saving the model completed in \" + str(save_time_taken/60) + \" minu\")\n",
    "\n",
    "    # # Saving the q values of the track states\n",
    "    def save_tracking_states(self):\n",
    "        q_value = agent.model.predict(np.array(env.state_encod_arch1(self.state_to_track)).reshape(1,36))\n",
    "        self.state_tracking.append(q_value[0][self.action_index_to_track])                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "f01cd0cf-aa9f-4499-b55f-603506c6830a",
    "_uuid": "f328c6c3-2970-4436-af71-59e4c4812ebb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Episodes = 15000\n",
    "write_threshold = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0c6fe4d-9d18-4454-aa76-e4b1e2c1ca7a",
    "_uuid": "20ef9e64-29ff-4645-b8dc-762271f90e0f"
   },
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "63882afa-87d8-4677-aa06-2503d86d0ec9",
    "_uuid": "0377131a-bc92-4e67-a955-5a857511ec88",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward 89.0, memory_length 143, epsilon 1.0\n",
      "Saving the model completed in 0.0003416021664937337 minu\n",
      "Writing the pickle file completed in 5.662441253662109e-06 minutes\n",
      "episode 500, reward -189.0, memory_length 2000, epsilon 0.7788029950635742\n",
      "episode 1000, reward -107.0, memory_length 2000, epsilon 0.6065345944060363\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "state_size = 36\n",
    "action_size = 21\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    score = 0\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    env = CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    (action_space, state_space, state) = env.reset()\n",
    "    terminal_state = False\n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    count = 0\n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        count+=1\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action_index = agent.get_action(state, episode)\n",
    "        action = action_space[action_index]\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward = env.reward_func(state, action, Time_matrix)\n",
    "        next_state, terminal_state = env.next_state_func(state, action, Time_matrix) \n",
    "        # 3. Append the experience to the memory\n",
    "        agent.append_sample(state, action_index, reward, next_state, terminal_state)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        if count%20 == 0:\n",
    "            agent.train_model()\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        score += reward\n",
    "        state = next_state\n",
    "    \n",
    "    # Store total rewards obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "    # epsilon decay\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "#         agent.epsilon *= agent.epsilon_decay\n",
    "        agent.epsilon = agent.epsilon_min + (agent.epsilon_max - agent.epsilon_min) * np.exp(agent.epsilon_decay*episode)\n",
    "    \n",
    "    # Logging every episode\n",
    "#     print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}\".format(episode, score, len(agent.memory), agent.epsilon))\n",
    "    \n",
    "    # Initialising the State_track dictionary to track the state\n",
    "#     if (episode == track_threshold-1):\n",
    "#         initialise_tracking_states()\n",
    "\n",
    "    if (episode%500 ==0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}\".format(episode, score, len(agent.memory), agent.epsilon))        \n",
    "    \n",
    "    if (episode%10 ==0):\n",
    "        agent.save_tracking_states()\n",
    "\n",
    "    # Every few episodes\n",
    "    if episode%write_threshold == 0:\n",
    "        # Saving the models\n",
    "        agent.save(\"model_weights.h5\")\n",
    "        # Saving the rewards per episode as a pickle file\n",
    "        save_pickle(rewards_per_episode,\"rewards_per_episode\")\n",
    "\n",
    "##### Simulation ends ######\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time/3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57796a49-f5fa-41dc-9394-ae7a22eb3ef7",
    "_uuid": "609d36a4-f07d-4eb9-b986-3764adbad99f"
   },
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb11f917-1b54-42b4-93fc-eaa0b684987b",
    "_uuid": "4ad8c490-1008-410f-8561-3c2d14ddd6a1",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with open(\"rewards_per_episode.pkl\", \"rb\") as f:\n",
    "    rewards_per_episode = pickle.load(f)\n",
    "\n",
    "plt.plot(list(range(len(rewards_per_episode))), rewards_per_episode)\n",
    "plt.xlabel(\"episode number\")\n",
    "plt.ylabel(\"rewards per episode\")\n",
    "plt.savefig('rewards.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.remove(\"./model_weights.h5\")\n",
    "# os.remove(\"./rewards_per_episode.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3dd2aa28-35ec-470b-a1be-ce6e4e3b7551",
    "_uuid": "b6301198-c23a-425c-9715-8bc2557b6a35"
   },
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04efc647-9226-41ee-ab2d-b4ac4ee5e569",
    "_uuid": "bba09de4-e91e-4f13-add9-9bb89d260490"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = -0.0005\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.epsilon = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f1274125-5c80-4720-84aa-015b6d4f1c40",
    "_uuid": "8244329d-3737-46d9-a226-823abd83f7ac",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "time = np.arange(0,15000)\n",
    "epsilon = []\n",
    "for i in range(0,15000):\n",
    "    epsilon.append(0.00001 + (1 - 0.00001) * np.exp(-0.0005*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f8389caa-942c-4f35-a4e7-0bae53f53787",
    "_uuid": "7da80223-bf90-4a30-91da-5a312de5d932",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "69407602-ca5f-4671-9ec3-5123e9d22125",
    "_uuid": "92a7fa52-93f3-4793-870a-06d2967ec631",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
